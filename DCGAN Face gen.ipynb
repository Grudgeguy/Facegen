{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f65928",
   "metadata": {},
   "source": [
    "Unsupervised representational learning using DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "165e73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import os \n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d2f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,channel_img,features_d):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channel_img,features_d,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d,features_d*2,4,2,1),\n",
    "            self._block(features_d*2,features_d*4,4,2,1),\n",
    "            self._block(features_d*4,features_d*8,4,2,1),\n",
    "            nn.Conv2d(features_d*8,1,4,2,0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "            \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding,bias=False):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e566b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def  __init__(self,channels_noise,img_dim,features_g):\n",
    "        super(Generator,self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(channels_noise,features_g*16,4,1,0),\n",
    "            self._block(features_g*16,features_g*8,4,2,1),\n",
    "            self._block(features_g*8,features_g*4,4,2,1),\n",
    "            self._block(features_g*4,features_g*2,4,2,1),\n",
    "            nn.ConvTranspose2d(features_g*2,img_dim,4,2,1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c29afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalizing weights of the model with mean 0 and std dev 1\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "#         print(m)\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N, in_channels, H, W = 8,3,64,64\n",
    "    noise =100\n",
    "    X = torch.randn((N,in_channels,H,W))\n",
    "    disc = Discriminator(in_channels,8)\n",
    "    initialize_weights(disc)\n",
    "    assert disc(X).shape == (N,1,1,1), \"Disc Failed\"\n",
    "    gen = Generator(noise,in_channels,8)\n",
    "    z = torch.randn((N,noise,1,1))\n",
    "    assert gen(z).shape == (N,in_channels,H,W), \"gen Failed\"\n",
    "    print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cb4c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db57b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nmax=32):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=32):\n",
    "    for images, _ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break\n",
    "\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8c9ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_size = 64\n",
    "features_gen =64\n",
    "features_disc = 64\n",
    "batch_size = 128\n",
    "noise_dim = 100\n",
    "epochs=60\n",
    "channels = 3\n",
    "lr=2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ce130b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(channels)], [0.5 for _ in range(channels)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8efe799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.MNIST(\n",
    "#     root=\"dataset/\", train=True, transform=transform, download=True\n",
    "# )\n",
    "\n",
    "dataset = ImageFolder(r\"C:\\Users\\dhruv\\Untitled Folder\\Fake Face Image Generator\\celeb_dataset\", transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "gen = Generator(noise_dim, channels, features_gen).to(device)\n",
    "disc = Discriminator(channels, features_disc).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9524bd3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2002dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'output_directory' with your desired directory path\n",
    "output_directory = 'generated_images_celeb'\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2541835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/60] Batch 0/1583                   Loss D: 0.7031, loss G: 0.7923\n",
      "Epoch [0/60] Batch 100/1583                   Loss D: 0.1970, loss G: 2.5234\n",
      "Epoch [0/60] Batch 200/1583                   Loss D: 0.7020, loss G: 2.0143\n",
      "Epoch [0/60] Batch 300/1583                   Loss D: 0.4471, loss G: 1.8668\n",
      "Epoch [0/60] Batch 400/1583                   Loss D: 0.5283, loss G: 1.9138\n",
      "Epoch [0/60] Batch 500/1583                   Loss D: 0.4387, loss G: 2.2269\n",
      "Epoch [0/60] Batch 600/1583                   Loss D: 0.4932, loss G: 1.8804\n",
      "Epoch [0/60] Batch 700/1583                   Loss D: 0.5949, loss G: 2.7632\n",
      "Epoch [0/60] Batch 800/1583                   Loss D: 0.4250, loss G: 2.7383\n",
      "Epoch [0/60] Batch 900/1583                   Loss D: 0.4614, loss G: 1.8955\n",
      "Epoch [0/60] Batch 1000/1583                   Loss D: 0.5312, loss G: 2.7696\n",
      "Epoch [0/60] Batch 1100/1583                   Loss D: 0.5805, loss G: 1.6545\n",
      "Epoch [0/60] Batch 1200/1583                   Loss D: 0.3902, loss G: 2.2205\n",
      "Epoch [0/60] Batch 1300/1583                   Loss D: 0.7843, loss G: 3.3798\n",
      "Epoch [0/60] Batch 1400/1583                   Loss D: 0.5723, loss G: 1.2640\n",
      "Epoch [0/60] Batch 1500/1583                   Loss D: 0.4227, loss G: 1.6365\n",
      "Epoch [1/60] Batch 0/1583                   Loss D: 0.5220, loss G: 1.9913\n",
      "Epoch [1/60] Batch 100/1583                   Loss D: 0.4771, loss G: 1.8981\n",
      "Epoch [1/60] Batch 200/1583                   Loss D: 0.4128, loss G: 2.1525\n",
      "Epoch [1/60] Batch 300/1583                   Loss D: 0.4967, loss G: 2.2608\n",
      "Epoch [1/60] Batch 400/1583                   Loss D: 0.4374, loss G: 1.7826\n",
      "Epoch [1/60] Batch 500/1583                   Loss D: 0.4575, loss G: 2.0820\n",
      "Epoch [1/60] Batch 600/1583                   Loss D: 0.5133, loss G: 1.4833\n",
      "Epoch [1/60] Batch 700/1583                   Loss D: 0.6035, loss G: 2.3413\n",
      "Epoch [1/60] Batch 800/1583                   Loss D: 0.6525, loss G: 2.7616\n",
      "Epoch [1/60] Batch 900/1583                   Loss D: 0.4642, loss G: 1.7563\n",
      "Epoch [1/60] Batch 1000/1583                   Loss D: 0.5009, loss G: 2.3105\n",
      "Epoch [1/60] Batch 1100/1583                   Loss D: 0.5195, loss G: 1.9859\n",
      "Epoch [1/60] Batch 1200/1583                   Loss D: 0.6151, loss G: 0.7436\n",
      "Epoch [1/60] Batch 1300/1583                   Loss D: 0.5670, loss G: 1.1437\n",
      "Epoch [1/60] Batch 1400/1583                   Loss D: 0.5795, loss G: 2.4745\n",
      "Epoch [1/60] Batch 1500/1583                   Loss D: 0.5469, loss G: 1.6394\n",
      "Epoch [2/60] Batch 0/1583                   Loss D: 0.4507, loss G: 1.2614\n",
      "Epoch [2/60] Batch 100/1583                   Loss D: 0.4895, loss G: 1.4083\n",
      "Epoch [2/60] Batch 200/1583                   Loss D: 0.5131, loss G: 1.3005\n",
      "Epoch [2/60] Batch 300/1583                   Loss D: 0.6217, loss G: 2.1383\n",
      "Epoch [2/60] Batch 400/1583                   Loss D: 0.4799, loss G: 1.4841\n",
      "Epoch [2/60] Batch 500/1583                   Loss D: 0.5322, loss G: 1.8398\n",
      "Epoch [2/60] Batch 600/1583                   Loss D: 0.4999, loss G: 1.9219\n",
      "Epoch [2/60] Batch 700/1583                   Loss D: 0.5427, loss G: 1.0567\n",
      "Epoch [2/60] Batch 800/1583                   Loss D: 0.5551, loss G: 2.0414\n",
      "Epoch [2/60] Batch 900/1583                   Loss D: 0.5232, loss G: 1.4609\n",
      "Epoch [2/60] Batch 1000/1583                   Loss D: 0.5011, loss G: 1.3118\n",
      "Epoch [2/60] Batch 1100/1583                   Loss D: 0.5793, loss G: 1.8648\n",
      "Epoch [2/60] Batch 1200/1583                   Loss D: 0.5579, loss G: 1.0001\n",
      "Epoch [2/60] Batch 1300/1583                   Loss D: 0.5683, loss G: 1.5237\n",
      "Epoch [2/60] Batch 1400/1583                   Loss D: 0.5651, loss G: 0.7320\n",
      "Epoch [2/60] Batch 1500/1583                   Loss D: 0.5049, loss G: 1.1924\n",
      "Epoch [3/60] Batch 0/1583                   Loss D: 0.5118, loss G: 1.5571\n",
      "Epoch [3/60] Batch 100/1583                   Loss D: 0.5294, loss G: 1.7383\n",
      "Epoch [3/60] Batch 200/1583                   Loss D: 0.5575, loss G: 2.3827\n",
      "Epoch [3/60] Batch 300/1583                   Loss D: 0.4936, loss G: 1.3581\n",
      "Epoch [3/60] Batch 400/1583                   Loss D: 0.5075, loss G: 0.8870\n",
      "Epoch [3/60] Batch 500/1583                   Loss D: 0.4724, loss G: 1.6734\n",
      "Epoch [3/60] Batch 600/1583                   Loss D: 0.5471, loss G: 1.3886\n",
      "Epoch [3/60] Batch 700/1583                   Loss D: 0.5018, loss G: 1.4989\n",
      "Epoch [3/60] Batch 800/1583                   Loss D: 0.5855, loss G: 2.7105\n",
      "Epoch [3/60] Batch 900/1583                   Loss D: 0.5074, loss G: 1.8248\n",
      "Epoch [3/60] Batch 1000/1583                   Loss D: 0.5322, loss G: 1.7643\n",
      "Epoch [3/60] Batch 1100/1583                   Loss D: 0.5359, loss G: 1.7669\n",
      "Epoch [3/60] Batch 1200/1583                   Loss D: 0.5682, loss G: 1.1929\n",
      "Epoch [3/60] Batch 1300/1583                   Loss D: 0.4802, loss G: 1.2877\n",
      "Epoch [3/60] Batch 1400/1583                   Loss D: 0.6181, loss G: 0.6385\n",
      "Epoch [3/60] Batch 1500/1583                   Loss D: 0.5098, loss G: 1.4909\n",
      "Epoch [4/60] Batch 0/1583                   Loss D: 0.5477, loss G: 1.4432\n",
      "Epoch [4/60] Batch 100/1583                   Loss D: 0.5229, loss G: 1.1796\n",
      "Epoch [4/60] Batch 200/1583                   Loss D: 0.5086, loss G: 2.0318\n",
      "Epoch [4/60] Batch 300/1583                   Loss D: 0.8182, loss G: 2.8448\n",
      "Epoch [4/60] Batch 400/1583                   Loss D: 0.5684, loss G: 1.5636\n",
      "Epoch [4/60] Batch 500/1583                   Loss D: 0.5184, loss G: 1.5544\n",
      "Epoch [4/60] Batch 600/1583                   Loss D: 0.4949, loss G: 1.3235\n",
      "Epoch [4/60] Batch 700/1583                   Loss D: 0.5493, loss G: 1.5294\n",
      "Epoch [4/60] Batch 800/1583                   Loss D: 0.4823, loss G: 1.7362\n",
      "Epoch [4/60] Batch 900/1583                   Loss D: 0.6386, loss G: 1.0621\n",
      "Epoch [4/60] Batch 1000/1583                   Loss D: 0.5895, loss G: 0.7249\n",
      "Epoch [4/60] Batch 1100/1583                   Loss D: 0.5026, loss G: 1.2582\n",
      "Epoch [4/60] Batch 1200/1583                   Loss D: 0.5208, loss G: 1.2350\n",
      "Epoch [4/60] Batch 1300/1583                   Loss D: 0.4888, loss G: 1.3792\n",
      "Epoch [4/60] Batch 1400/1583                   Loss D: 0.4850, loss G: 1.6050\n",
      "Epoch [4/60] Batch 1500/1583                   Loss D: 0.5557, loss G: 1.1838\n",
      "Epoch [5/60] Batch 0/1583                   Loss D: 0.7517, loss G: 1.6931\n",
      "Epoch [5/60] Batch 100/1583                   Loss D: 0.6028, loss G: 1.0792\n",
      "Epoch [5/60] Batch 200/1583                   Loss D: 0.5778, loss G: 1.3940\n",
      "Epoch [5/60] Batch 300/1583                   Loss D: 0.4684, loss G: 1.3958\n",
      "Epoch [5/60] Batch 400/1583                   Loss D: 0.8285, loss G: 0.6673\n",
      "Epoch [5/60] Batch 500/1583                   Loss D: 0.6292, loss G: 1.1895\n",
      "Epoch [5/60] Batch 600/1583                   Loss D: 0.4845, loss G: 1.3278\n",
      "Epoch [5/60] Batch 700/1583                   Loss D: 0.5260, loss G: 1.5109\n",
      "Epoch [5/60] Batch 800/1583                   Loss D: 0.4437, loss G: 1.2923\n",
      "Epoch [5/60] Batch 900/1583                   Loss D: 0.6209, loss G: 1.7009\n",
      "Epoch [5/60] Batch 1000/1583                   Loss D: 0.4885, loss G: 1.3569\n",
      "Epoch [5/60] Batch 1100/1583                   Loss D: 0.4595, loss G: 1.2837\n",
      "Epoch [5/60] Batch 1200/1583                   Loss D: 0.5501, loss G: 1.3432\n",
      "Epoch [5/60] Batch 1300/1583                   Loss D: 0.5738, loss G: 1.0123\n",
      "Epoch [5/60] Batch 1400/1583                   Loss D: 0.4699, loss G: 2.2338\n",
      "Epoch [5/60] Batch 1500/1583                   Loss D: 0.4945, loss G: 1.7225\n",
      "Epoch [6/60] Batch 0/1583                   Loss D: 0.4999, loss G: 1.9280\n",
      "Epoch [6/60] Batch 100/1583                   Loss D: 0.4792, loss G: 1.0558\n",
      "Epoch [6/60] Batch 200/1583                   Loss D: 0.4980, loss G: 1.2703\n",
      "Epoch [6/60] Batch 300/1583                   Loss D: 0.4471, loss G: 2.4279\n",
      "Epoch [6/60] Batch 400/1583                   Loss D: 0.5112, loss G: 1.3009\n",
      "Epoch [6/60] Batch 500/1583                   Loss D: 0.4999, loss G: 1.2080\n",
      "Epoch [6/60] Batch 600/1583                   Loss D: 0.4500, loss G: 1.4934\n",
      "Epoch [6/60] Batch 700/1583                   Loss D: 0.5394, loss G: 2.4685\n",
      "Epoch [6/60] Batch 800/1583                   Loss D: 0.5092, loss G: 0.9241\n",
      "Epoch [6/60] Batch 900/1583                   Loss D: 0.3096, loss G: 2.4834\n",
      "Epoch [6/60] Batch 1000/1583                   Loss D: 0.4287, loss G: 1.6573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/60] Batch 1100/1583                   Loss D: 0.4517, loss G: 1.2365\n",
      "Epoch [6/60] Batch 1200/1583                   Loss D: 0.4597, loss G: 0.9659\n",
      "Epoch [6/60] Batch 1300/1583                   Loss D: 0.4894, loss G: 1.3598\n",
      "Epoch [6/60] Batch 1400/1583                   Loss D: 0.5204, loss G: 0.9864\n",
      "Epoch [6/60] Batch 1500/1583                   Loss D: 0.5250, loss G: 2.3587\n",
      "Epoch [7/60] Batch 0/1583                   Loss D: 0.5777, loss G: 1.3001\n",
      "Epoch [7/60] Batch 100/1583                   Loss D: 0.4179, loss G: 1.4337\n",
      "Epoch [7/60] Batch 200/1583                   Loss D: 0.5126, loss G: 1.3247\n",
      "Epoch [7/60] Batch 300/1583                   Loss D: 0.3993, loss G: 1.3537\n",
      "Epoch [7/60] Batch 400/1583                   Loss D: 0.4899, loss G: 1.9841\n",
      "Epoch [7/60] Batch 500/1583                   Loss D: 0.4040, loss G: 1.9066\n",
      "Epoch [7/60] Batch 600/1583                   Loss D: 0.4227, loss G: 1.8564\n",
      "Epoch [7/60] Batch 700/1583                   Loss D: 0.4925, loss G: 0.9496\n",
      "Epoch [7/60] Batch 800/1583                   Loss D: 0.6476, loss G: 0.7215\n",
      "Epoch [7/60] Batch 900/1583                   Loss D: 0.5053, loss G: 1.4517\n",
      "Epoch [7/60] Batch 1000/1583                   Loss D: 0.4062, loss G: 1.7928\n",
      "Epoch [7/60] Batch 1100/1583                   Loss D: 0.5160, loss G: 2.8103\n",
      "Epoch [7/60] Batch 1200/1583                   Loss D: 0.5144, loss G: 1.3291\n",
      "Epoch [7/60] Batch 1300/1583                   Loss D: 0.5046, loss G: 1.1082\n",
      "Epoch [7/60] Batch 1400/1583                   Loss D: 0.3121, loss G: 2.0896\n",
      "Epoch [7/60] Batch 1500/1583                   Loss D: 0.4548, loss G: 1.8204\n",
      "Epoch [8/60] Batch 0/1583                   Loss D: 0.4127, loss G: 1.6585\n",
      "Epoch [8/60] Batch 100/1583                   Loss D: 0.4588, loss G: 0.9828\n",
      "Epoch [8/60] Batch 200/1583                   Loss D: 0.4127, loss G: 1.2970\n",
      "Epoch [8/60] Batch 300/1583                   Loss D: 0.4030, loss G: 2.0528\n",
      "Epoch [8/60] Batch 400/1583                   Loss D: 0.4071, loss G: 1.6327\n",
      "Epoch [8/60] Batch 500/1583                   Loss D: 0.3639, loss G: 1.9708\n",
      "Epoch [8/60] Batch 600/1583                   Loss D: 0.7061, loss G: 0.7375\n",
      "Epoch [8/60] Batch 700/1583                   Loss D: 0.4275, loss G: 1.3644\n",
      "Epoch [8/60] Batch 800/1583                   Loss D: 0.4457, loss G: 1.3108\n",
      "Epoch [8/60] Batch 900/1583                   Loss D: 0.3782, loss G: 1.4886\n",
      "Epoch [8/60] Batch 1000/1583                   Loss D: 0.4446, loss G: 3.2466\n",
      "Epoch [8/60] Batch 1100/1583                   Loss D: 0.4680, loss G: 1.6664\n",
      "Epoch [8/60] Batch 1200/1583                   Loss D: 0.4091, loss G: 2.4198\n",
      "Epoch [8/60] Batch 1300/1583                   Loss D: 0.4352, loss G: 1.0606\n",
      "Epoch [8/60] Batch 1400/1583                   Loss D: 0.3648, loss G: 2.1875\n",
      "Epoch [8/60] Batch 1500/1583                   Loss D: 0.3393, loss G: 1.5078\n",
      "Epoch [9/60] Batch 0/1583                   Loss D: 0.3990, loss G: 1.5538\n",
      "Epoch [9/60] Batch 100/1583                   Loss D: 0.4009, loss G: 1.6837\n",
      "Epoch [9/60] Batch 200/1583                   Loss D: 0.2041, loss G: 2.4891\n",
      "Epoch [9/60] Batch 300/1583                   Loss D: 0.3745, loss G: 2.1559\n",
      "Epoch [9/60] Batch 400/1583                   Loss D: 0.3690, loss G: 2.2825\n",
      "Epoch [9/60] Batch 500/1583                   Loss D: 0.5744, loss G: 0.9516\n",
      "Epoch [9/60] Batch 600/1583                   Loss D: 0.3509, loss G: 1.7910\n",
      "Epoch [9/60] Batch 700/1583                   Loss D: 0.3543, loss G: 1.6183\n",
      "Epoch [9/60] Batch 800/1583                   Loss D: 0.2959, loss G: 1.8812\n",
      "Epoch [9/60] Batch 900/1583                   Loss D: 0.3511, loss G: 2.3298\n",
      "Epoch [9/60] Batch 1000/1583                   Loss D: 0.4036, loss G: 1.5987\n",
      "Epoch [9/60] Batch 1100/1583                   Loss D: 0.4346, loss G: 2.8496\n",
      "Epoch [9/60] Batch 1200/1583                   Loss D: 0.5052, loss G: 0.9886\n",
      "Epoch [9/60] Batch 1300/1583                   Loss D: 0.2971, loss G: 1.7292\n",
      "Epoch [9/60] Batch 1400/1583                   Loss D: 1.0682, loss G: 3.0881\n",
      "Epoch [9/60] Batch 1500/1583                   Loss D: 0.3150, loss G: 2.3148\n",
      "Epoch [10/60] Batch 0/1583                   Loss D: 0.3318, loss G: 1.7977\n",
      "Epoch [10/60] Batch 100/1583                   Loss D: 0.2639, loss G: 1.8089\n",
      "Epoch [10/60] Batch 200/1583                   Loss D: 0.3220, loss G: 1.9644\n",
      "Epoch [10/60] Batch 300/1583                   Loss D: 0.4731, loss G: 0.9598\n",
      "Epoch [10/60] Batch 400/1583                   Loss D: 0.3881, loss G: 1.6709\n",
      "Epoch [10/60] Batch 500/1583                   Loss D: 0.3427, loss G: 1.8428\n",
      "Epoch [10/60] Batch 600/1583                   Loss D: 0.3002, loss G: 2.5007\n",
      "Epoch [10/60] Batch 700/1583                   Loss D: 0.4047, loss G: 1.1245\n",
      "Epoch [10/60] Batch 800/1583                   Loss D: 0.3341, loss G: 2.2548\n",
      "Epoch [10/60] Batch 900/1583                   Loss D: 0.4240, loss G: 2.5283\n",
      "Epoch [10/60] Batch 1000/1583                   Loss D: 0.2658, loss G: 1.7016\n",
      "Epoch [10/60] Batch 1100/1583                   Loss D: 0.3378, loss G: 1.2768\n",
      "Epoch [10/60] Batch 1200/1583                   Loss D: 0.3106, loss G: 2.5708\n",
      "Epoch [10/60] Batch 1300/1583                   Loss D: 0.3032, loss G: 1.9220\n",
      "Epoch [10/60] Batch 1400/1583                   Loss D: 0.4164, loss G: 1.4330\n",
      "Epoch [10/60] Batch 1500/1583                   Loss D: 3.7811, loss G: 4.1577\n",
      "Epoch [11/60] Batch 0/1583                   Loss D: 0.3619, loss G: 1.6404\n",
      "Epoch [11/60] Batch 100/1583                   Loss D: 0.2969, loss G: 1.4710\n",
      "Epoch [11/60] Batch 200/1583                   Loss D: 0.2294, loss G: 2.7836\n",
      "Epoch [11/60] Batch 300/1583                   Loss D: 0.2992, loss G: 2.8577\n",
      "Epoch [11/60] Batch 400/1583                   Loss D: 0.2995, loss G: 1.9396\n",
      "Epoch [11/60] Batch 500/1583                   Loss D: 0.2949, loss G: 2.5510\n",
      "Epoch [11/60] Batch 600/1583                   Loss D: 0.3726, loss G: 3.2087\n",
      "Epoch [11/60] Batch 700/1583                   Loss D: 0.1790, loss G: 2.8302\n",
      "Epoch [11/60] Batch 800/1583                   Loss D: 0.5932, loss G: 1.6241\n",
      "Epoch [11/60] Batch 900/1583                   Loss D: 0.3388, loss G: 1.6042\n",
      "Epoch [11/60] Batch 1000/1583                   Loss D: 0.3831, loss G: 2.3616\n",
      "Epoch [11/60] Batch 1100/1583                   Loss D: 0.4193, loss G: 2.9567\n",
      "Epoch [11/60] Batch 1200/1583                   Loss D: 0.2335, loss G: 2.3595\n",
      "Epoch [11/60] Batch 1300/1583                   Loss D: 0.2564, loss G: 3.1580\n",
      "Epoch [11/60] Batch 1400/1583                   Loss D: 0.4103, loss G: 3.1281\n",
      "Epoch [11/60] Batch 1500/1583                   Loss D: 0.2717, loss G: 2.1593\n",
      "Epoch [12/60] Batch 0/1583                   Loss D: 0.3045, loss G: 3.1320\n",
      "Epoch [12/60] Batch 100/1583                   Loss D: 0.3942, loss G: 1.0307\n",
      "Epoch [12/60] Batch 200/1583                   Loss D: 0.2800, loss G: 2.6486\n",
      "Epoch [12/60] Batch 300/1583                   Loss D: 0.2873, loss G: 2.2424\n",
      "Epoch [12/60] Batch 400/1583                   Loss D: 0.1916, loss G: 1.9118\n",
      "Epoch [12/60] Batch 500/1583                   Loss D: 0.3104, loss G: 2.3384\n",
      "Epoch [12/60] Batch 600/1583                   Loss D: 0.4843, loss G: 1.3010\n",
      "Epoch [12/60] Batch 700/1583                   Loss D: 0.2699, loss G: 1.8128\n",
      "Epoch [12/60] Batch 800/1583                   Loss D: 0.3228, loss G: 2.5504\n",
      "Epoch [12/60] Batch 900/1583                   Loss D: 0.2325, loss G: 2.1838\n",
      "Epoch [12/60] Batch 1000/1583                   Loss D: 0.1898, loss G: 3.0619\n",
      "Epoch [12/60] Batch 1100/1583                   Loss D: 0.6076, loss G: 4.5001\n",
      "Epoch [12/60] Batch 1200/1583                   Loss D: 0.3793, loss G: 1.8642\n",
      "Epoch [12/60] Batch 1300/1583                   Loss D: 0.2548, loss G: 2.9747\n",
      "Epoch [12/60] Batch 1400/1583                   Loss D: 0.2980, loss G: 1.4677\n",
      "Epoch [12/60] Batch 1500/1583                   Loss D: 0.2038, loss G: 2.8478\n",
      "Epoch [13/60] Batch 0/1583                   Loss D: 0.3246, loss G: 1.7748\n",
      "Epoch [13/60] Batch 100/1583                   Loss D: 0.4221, loss G: 2.1497\n",
      "Epoch [13/60] Batch 200/1583                   Loss D: 0.2335, loss G: 2.0874\n",
      "Epoch [13/60] Batch 300/1583                   Loss D: 0.2375, loss G: 1.5405\n",
      "Epoch [13/60] Batch 400/1583                   Loss D: 0.4672, loss G: 4.0648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/60] Batch 500/1583                   Loss D: 0.2555, loss G: 2.5009\n",
      "Epoch [13/60] Batch 600/1583                   Loss D: 0.2802, loss G: 2.8691\n",
      "Epoch [13/60] Batch 700/1583                   Loss D: 0.3607, loss G: 4.4400\n",
      "Epoch [13/60] Batch 800/1583                   Loss D: 0.2559, loss G: 2.0844\n",
      "Epoch [13/60] Batch 900/1583                   Loss D: 0.3143, loss G: 4.2523\n",
      "Epoch [13/60] Batch 1000/1583                   Loss D: 1.2595, loss G: 0.8554\n",
      "Epoch [13/60] Batch 1100/1583                   Loss D: 0.4009, loss G: 2.1441\n",
      "Epoch [13/60] Batch 1200/1583                   Loss D: 0.2580, loss G: 2.9516\n",
      "Epoch [13/60] Batch 1300/1583                   Loss D: 0.2410, loss G: 3.4109\n",
      "Epoch [13/60] Batch 1400/1583                   Loss D: 0.2307, loss G: 2.3325\n",
      "Epoch [13/60] Batch 1500/1583                   Loss D: 0.1684, loss G: 1.8872\n",
      "Epoch [14/60] Batch 0/1583                   Loss D: 0.1417, loss G: 2.7801\n",
      "Epoch [14/60] Batch 100/1583                   Loss D: 0.2549, loss G: 1.6898\n",
      "Epoch [14/60] Batch 200/1583                   Loss D: 0.1732, loss G: 2.7127\n",
      "Epoch [14/60] Batch 300/1583                   Loss D: 0.2022, loss G: 2.7780\n",
      "Epoch [14/60] Batch 400/1583                   Loss D: 0.2000, loss G: 2.2287\n",
      "Epoch [14/60] Batch 500/1583                   Loss D: 0.2344, loss G: 2.0676\n",
      "Epoch [14/60] Batch 600/1583                   Loss D: 0.2393, loss G: 1.4886\n",
      "Epoch [14/60] Batch 700/1583                   Loss D: 0.1928, loss G: 2.3267\n",
      "Epoch [14/60] Batch 800/1583                   Loss D: 0.2501, loss G: 2.0189\n",
      "Epoch [14/60] Batch 900/1583                   Loss D: 0.2895, loss G: 1.9509\n",
      "Epoch [14/60] Batch 1000/1583                   Loss D: 0.2604, loss G: 1.8387\n",
      "Epoch [14/60] Batch 1100/1583                   Loss D: 0.2635, loss G: 4.0171\n",
      "Epoch [14/60] Batch 1200/1583                   Loss D: 0.2955, loss G: 4.2315\n",
      "Epoch [14/60] Batch 1300/1583                   Loss D: 0.7399, loss G: 6.1250\n",
      "Epoch [14/60] Batch 1400/1583                   Loss D: 0.2320, loss G: 2.7494\n",
      "Epoch [14/60] Batch 1500/1583                   Loss D: 0.2482, loss G: 2.9301\n",
      "Epoch [15/60] Batch 0/1583                   Loss D: 0.1248, loss G: 3.6187\n",
      "Epoch [15/60] Batch 100/1583                   Loss D: 0.1988, loss G: 2.2497\n",
      "Epoch [15/60] Batch 200/1583                   Loss D: 0.2863, loss G: 4.3238\n",
      "Epoch [15/60] Batch 300/1583                   Loss D: 0.3889, loss G: 0.9275\n",
      "Epoch [15/60] Batch 400/1583                   Loss D: 0.2179, loss G: 2.3966\n",
      "Epoch [15/60] Batch 500/1583                   Loss D: 0.3579, loss G: 5.8870\n",
      "Epoch [15/60] Batch 600/1583                   Loss D: 0.1575, loss G: 3.0889\n",
      "Epoch [15/60] Batch 700/1583                   Loss D: 0.1797, loss G: 2.3773\n",
      "Epoch [15/60] Batch 800/1583                   Loss D: 0.1639, loss G: 3.2187\n",
      "Epoch [15/60] Batch 900/1583                   Loss D: 0.2677, loss G: 2.2505\n",
      "Epoch [15/60] Batch 1000/1583                   Loss D: 0.1775, loss G: 2.4409\n",
      "Epoch [15/60] Batch 1100/1583                   Loss D: 0.1946, loss G: 2.0309\n",
      "Epoch [15/60] Batch 1200/1583                   Loss D: 0.1761, loss G: 2.1667\n",
      "Epoch [15/60] Batch 1300/1583                   Loss D: 0.2370, loss G: 3.5152\n",
      "Epoch [15/60] Batch 1400/1583                   Loss D: 0.2864, loss G: 4.5217\n",
      "Epoch [15/60] Batch 1500/1583                   Loss D: 0.4319, loss G: 1.0248\n",
      "Epoch [16/60] Batch 0/1583                   Loss D: 0.2611, loss G: 3.6070\n",
      "Epoch [16/60] Batch 100/1583                   Loss D: 0.2001, loss G: 3.8956\n",
      "Epoch [16/60] Batch 200/1583                   Loss D: 0.1493, loss G: 3.3292\n",
      "Epoch [16/60] Batch 300/1583                   Loss D: 0.2746, loss G: 3.3590\n",
      "Epoch [16/60] Batch 400/1583                   Loss D: 0.2121, loss G: 3.3648\n",
      "Epoch [16/60] Batch 500/1583                   Loss D: 0.1505, loss G: 2.9334\n",
      "Epoch [16/60] Batch 600/1583                   Loss D: 0.3073, loss G: 3.2962\n",
      "Epoch [16/60] Batch 700/1583                   Loss D: 0.2427, loss G: 3.8262\n",
      "Epoch [16/60] Batch 800/1583                   Loss D: 0.3073, loss G: 5.0688\n",
      "Epoch [16/60] Batch 900/1583                   Loss D: 0.3017, loss G: 4.1606\n",
      "Epoch [16/60] Batch 1000/1583                   Loss D: 0.1731, loss G: 3.1472\n",
      "Epoch [16/60] Batch 1100/1583                   Loss D: 0.2056, loss G: 2.8337\n",
      "Epoch [16/60] Batch 1200/1583                   Loss D: 0.1376, loss G: 2.8916\n",
      "Epoch [16/60] Batch 1300/1583                   Loss D: 0.2280, loss G: 3.2997\n",
      "Epoch [16/60] Batch 1400/1583                   Loss D: 0.2395, loss G: 4.0232\n",
      "Epoch [16/60] Batch 1500/1583                   Loss D: 0.2552, loss G: 1.8581\n",
      "Epoch [17/60] Batch 0/1583                   Loss D: 0.3117, loss G: 5.2558\n",
      "Epoch [17/60] Batch 100/1583                   Loss D: 0.1957, loss G: 4.4547\n",
      "Epoch [17/60] Batch 200/1583                   Loss D: 0.1588, loss G: 2.3335\n",
      "Epoch [17/60] Batch 300/1583                   Loss D: 0.5775, loss G: 5.7509\n",
      "Epoch [17/60] Batch 400/1583                   Loss D: 0.4603, loss G: 1.1222\n",
      "Epoch [17/60] Batch 500/1583                   Loss D: 0.2554, loss G: 1.4403\n",
      "Epoch [17/60] Batch 600/1583                   Loss D: 0.2249, loss G: 2.1045\n",
      "Epoch [17/60] Batch 700/1583                   Loss D: 0.1545, loss G: 2.0618\n",
      "Epoch [17/60] Batch 800/1583                   Loss D: 0.1987, loss G: 3.6256\n",
      "Epoch [17/60] Batch 900/1583                   Loss D: 0.1460, loss G: 2.7738\n",
      "Epoch [17/60] Batch 1000/1583                   Loss D: 0.1845, loss G: 3.0252\n",
      "Epoch [17/60] Batch 1100/1583                   Loss D: 0.1566, loss G: 2.8481\n",
      "Epoch [17/60] Batch 1200/1583                   Loss D: 0.1380, loss G: 3.3799\n",
      "Epoch [17/60] Batch 1300/1583                   Loss D: 0.1446, loss G: 3.1911\n",
      "Epoch [17/60] Batch 1400/1583                   Loss D: 0.2249, loss G: 5.0901\n",
      "Epoch [17/60] Batch 1500/1583                   Loss D: 0.2070, loss G: 1.9707\n",
      "Epoch [18/60] Batch 0/1583                   Loss D: 0.1583, loss G: 3.1811\n",
      "Epoch [18/60] Batch 100/1583                   Loss D: 0.1450, loss G: 2.9651\n",
      "Epoch [18/60] Batch 200/1583                   Loss D: 0.2328, loss G: 3.0760\n",
      "Epoch [18/60] Batch 300/1583                   Loss D: 0.1451, loss G: 3.0991\n",
      "Epoch [18/60] Batch 400/1583                   Loss D: 0.2123, loss G: 3.0472\n",
      "Epoch [18/60] Batch 500/1583                   Loss D: 0.1481, loss G: 3.3481\n",
      "Epoch [18/60] Batch 600/1583                   Loss D: 0.2373, loss G: 1.3656\n",
      "Epoch [18/60] Batch 700/1583                   Loss D: 0.7300, loss G: 4.1689\n",
      "Epoch [18/60] Batch 800/1583                   Loss D: 0.2621, loss G: 1.9256\n",
      "Epoch [18/60] Batch 900/1583                   Loss D: 0.1109, loss G: 3.3235\n",
      "Epoch [18/60] Batch 1000/1583                   Loss D: 0.2085, loss G: 2.0005\n",
      "Epoch [18/60] Batch 1100/1583                   Loss D: 0.0959, loss G: 2.7690\n",
      "Epoch [18/60] Batch 1200/1583                   Loss D: 0.1103, loss G: 3.6738\n",
      "Epoch [18/60] Batch 1300/1583                   Loss D: 0.2431, loss G: 5.3609\n",
      "Epoch [18/60] Batch 1400/1583                   Loss D: 1.4305, loss G: 0.4557\n",
      "Epoch [18/60] Batch 1500/1583                   Loss D: 0.1733, loss G: 4.7005\n",
      "Epoch [19/60] Batch 0/1583                   Loss D: 0.1784, loss G: 2.4882\n",
      "Epoch [19/60] Batch 100/1583                   Loss D: 0.1413, loss G: 2.7216\n",
      "Epoch [19/60] Batch 200/1583                   Loss D: 0.1162, loss G: 2.3359\n",
      "Epoch [19/60] Batch 300/1583                   Loss D: 0.1564, loss G: 3.3143\n",
      "Epoch [19/60] Batch 400/1583                   Loss D: 0.1270, loss G: 3.7900\n",
      "Epoch [19/60] Batch 500/1583                   Loss D: 0.2375, loss G: 3.8160\n",
      "Epoch [19/60] Batch 600/1583                   Loss D: 0.0929, loss G: 2.9786\n",
      "Epoch [19/60] Batch 700/1583                   Loss D: 0.2486, loss G: 3.6469\n",
      "Epoch [19/60] Batch 800/1583                   Loss D: 0.4677, loss G: 5.8379\n",
      "Epoch [19/60] Batch 900/1583                   Loss D: 0.1756, loss G: 2.8213\n",
      "Epoch [19/60] Batch 1000/1583                   Loss D: 0.6710, loss G: 6.6229\n",
      "Epoch [19/60] Batch 1100/1583                   Loss D: 0.0911, loss G: 3.0845\n",
      "Epoch [19/60] Batch 1200/1583                   Loss D: 0.1580, loss G: 2.7127\n",
      "Epoch [19/60] Batch 1300/1583                   Loss D: 0.1139, loss G: 3.3485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/60] Batch 1400/1583                   Loss D: 0.1885, loss G: 4.1249\n",
      "Epoch [19/60] Batch 1500/1583                   Loss D: 0.1616, loss G: 2.6096\n",
      "Epoch [20/60] Batch 0/1583                   Loss D: 1.3702, loss G: 5.0757\n",
      "Epoch [20/60] Batch 100/1583                   Loss D: 0.1859, loss G: 3.3347\n",
      "Epoch [20/60] Batch 200/1583                   Loss D: 0.2741, loss G: 1.1927\n",
      "Epoch [20/60] Batch 300/1583                   Loss D: 0.1149, loss G: 4.2487\n",
      "Epoch [20/60] Batch 400/1583                   Loss D: 0.2022, loss G: 2.8287\n",
      "Epoch [20/60] Batch 500/1583                   Loss D: 0.1015, loss G: 3.8814\n",
      "Epoch [20/60] Batch 600/1583                   Loss D: 0.1125, loss G: 2.9151\n",
      "Epoch [20/60] Batch 700/1583                   Loss D: 0.1263, loss G: 3.0539\n",
      "Epoch [20/60] Batch 800/1583                   Loss D: 0.1323, loss G: 2.8643\n",
      "Epoch [20/60] Batch 900/1583                   Loss D: 0.2894, loss G: 1.4736\n",
      "Epoch [20/60] Batch 1000/1583                   Loss D: 0.2123, loss G: 3.6726\n",
      "Epoch [20/60] Batch 1100/1583                   Loss D: 0.2368, loss G: 1.6853\n",
      "Epoch [20/60] Batch 1200/1583                   Loss D: 0.1924, loss G: 4.5741\n",
      "Epoch [20/60] Batch 1300/1583                   Loss D: 0.0808, loss G: 3.2522\n",
      "Epoch [20/60] Batch 1400/1583                   Loss D: 0.3159, loss G: 4.3491\n",
      "Epoch [20/60] Batch 1500/1583                   Loss D: 0.2767, loss G: 4.1760\n",
      "Epoch [21/60] Batch 0/1583                   Loss D: 0.3801, loss G: 6.7895\n",
      "Epoch [21/60] Batch 100/1583                   Loss D: 0.2690, loss G: 1.1841\n",
      "Epoch [21/60] Batch 200/1583                   Loss D: 0.0941, loss G: 3.3804\n",
      "Epoch [21/60] Batch 300/1583                   Loss D: 0.1129, loss G: 3.8142\n",
      "Epoch [21/60] Batch 400/1583                   Loss D: 0.5666, loss G: 6.5708\n",
      "Epoch [21/60] Batch 500/1583                   Loss D: 0.1199, loss G: 4.2158\n",
      "Epoch [21/60] Batch 600/1583                   Loss D: 0.1410, loss G: 2.9734\n",
      "Epoch [21/60] Batch 700/1583                   Loss D: 0.1425, loss G: 2.7526\n",
      "Epoch [21/60] Batch 800/1583                   Loss D: 0.1496, loss G: 3.7917\n",
      "Epoch [21/60] Batch 900/1583                   Loss D: 0.1833, loss G: 1.7394\n",
      "Epoch [21/60] Batch 1000/1583                   Loss D: 0.0928, loss G: 2.8082\n",
      "Epoch [21/60] Batch 1100/1583                   Loss D: 0.1684, loss G: 4.3474\n",
      "Epoch [21/60] Batch 1200/1583                   Loss D: 0.0732, loss G: 3.4076\n",
      "Epoch [21/60] Batch 1300/1583                   Loss D: 0.1755, loss G: 1.5428\n",
      "Epoch [21/60] Batch 1400/1583                   Loss D: 0.6522, loss G: 7.2827\n",
      "Epoch [21/60] Batch 1500/1583                   Loss D: 0.0736, loss G: 4.3929\n",
      "Epoch [22/60] Batch 0/1583                   Loss D: 0.2417, loss G: 3.1317\n",
      "Epoch [22/60] Batch 100/1583                   Loss D: 0.1490, loss G: 4.9931\n",
      "Epoch [22/60] Batch 200/1583                   Loss D: 0.2250, loss G: 3.5867\n",
      "Epoch [22/60] Batch 300/1583                   Loss D: 0.1172, loss G: 2.4358\n",
      "Epoch [22/60] Batch 400/1583                   Loss D: 0.1243, loss G: 4.0444\n",
      "Epoch [22/60] Batch 500/1583                   Loss D: 0.1225, loss G: 2.8347\n",
      "Epoch [22/60] Batch 600/1583                   Loss D: 0.1358, loss G: 2.3811\n",
      "Epoch [22/60] Batch 700/1583                   Loss D: 0.1853, loss G: 1.1262\n",
      "Epoch [22/60] Batch 800/1583                   Loss D: 0.0696, loss G: 3.7076\n",
      "Epoch [22/60] Batch 900/1583                   Loss D: 0.0941, loss G: 3.2928\n",
      "Epoch [22/60] Batch 1000/1583                   Loss D: 0.2777, loss G: 5.5130\n",
      "Epoch [22/60] Batch 1100/1583                   Loss D: 0.6276, loss G: 1.2352\n",
      "Epoch [22/60] Batch 1200/1583                   Loss D: 0.1953, loss G: 3.0690\n",
      "Epoch [22/60] Batch 1300/1583                   Loss D: 0.0880, loss G: 3.2724\n",
      "Epoch [22/60] Batch 1400/1583                   Loss D: 0.0960, loss G: 3.6731\n",
      "Epoch [22/60] Batch 1500/1583                   Loss D: 0.0626, loss G: 4.8715\n",
      "Epoch [23/60] Batch 0/1583                   Loss D: 0.1331, loss G: 3.9488\n",
      "Epoch [23/60] Batch 100/1583                   Loss D: 0.1022, loss G: 3.3002\n",
      "Epoch [23/60] Batch 200/1583                   Loss D: 0.1500, loss G: 3.1877\n",
      "Epoch [23/60] Batch 300/1583                   Loss D: 0.1935, loss G: 3.8970\n",
      "Epoch [23/60] Batch 400/1583                   Loss D: 0.0488, loss G: 4.8964\n",
      "Epoch [23/60] Batch 500/1583                   Loss D: 0.1242, loss G: 4.7255\n",
      "Epoch [23/60] Batch 600/1583                   Loss D: 0.0665, loss G: 4.2196\n",
      "Epoch [23/60] Batch 700/1583                   Loss D: 0.2002, loss G: 3.1490\n",
      "Epoch [23/60] Batch 800/1583                   Loss D: 0.1664, loss G: 1.9708\n",
      "Epoch [23/60] Batch 900/1583                   Loss D: 0.1680, loss G: 3.2401\n",
      "Epoch [23/60] Batch 1000/1583                   Loss D: 0.0855, loss G: 3.5983\n",
      "Epoch [23/60] Batch 1100/1583                   Loss D: 0.2313, loss G: 3.7246\n",
      "Epoch [23/60] Batch 1200/1583                   Loss D: 0.0881, loss G: 4.1648\n",
      "Epoch [23/60] Batch 1300/1583                   Loss D: 0.3256, loss G: 2.6705\n",
      "Epoch [23/60] Batch 1400/1583                   Loss D: 0.0852, loss G: 3.4416\n",
      "Epoch [23/60] Batch 1500/1583                   Loss D: 0.1283, loss G: 3.9515\n",
      "Epoch [24/60] Batch 0/1583                   Loss D: 0.0659, loss G: 3.9424\n",
      "Epoch [24/60] Batch 100/1583                   Loss D: 0.0650, loss G: 5.3184\n",
      "Epoch [24/60] Batch 200/1583                   Loss D: 1.4780, loss G: 0.7339\n",
      "Epoch [24/60] Batch 300/1583                   Loss D: 0.0765, loss G: 3.1603\n",
      "Epoch [24/60] Batch 400/1583                   Loss D: 0.3321, loss G: 2.3981\n",
      "Epoch [24/60] Batch 500/1583                   Loss D: 0.3887, loss G: 0.5361\n",
      "Epoch [24/60] Batch 600/1583                   Loss D: 0.0880, loss G: 3.6614\n",
      "Epoch [24/60] Batch 700/1583                   Loss D: 0.2520, loss G: 2.8309\n",
      "Epoch [24/60] Batch 800/1583                   Loss D: 0.1114, loss G: 2.9199\n",
      "Epoch [24/60] Batch 900/1583                   Loss D: 0.0993, loss G: 3.0000\n",
      "Epoch [24/60] Batch 1000/1583                   Loss D: 0.3449, loss G: 3.5312\n",
      "Epoch [24/60] Batch 1100/1583                   Loss D: 0.1349, loss G: 2.1808\n",
      "Epoch [24/60] Batch 1200/1583                   Loss D: 0.1213, loss G: 2.9449\n",
      "Epoch [24/60] Batch 1300/1583                   Loss D: 0.0697, loss G: 3.5269\n",
      "Epoch [24/60] Batch 1400/1583                   Loss D: 0.2165, loss G: 1.7039\n",
      "Epoch [24/60] Batch 1500/1583                   Loss D: 1.0543, loss G: 0.0970\n",
      "Epoch [25/60] Batch 0/1583                   Loss D: 0.1293, loss G: 3.6549\n",
      "Epoch [25/60] Batch 100/1583                   Loss D: 0.0545, loss G: 3.5483\n",
      "Epoch [25/60] Batch 200/1583                   Loss D: 0.5515, loss G: 3.4386\n",
      "Epoch [25/60] Batch 300/1583                   Loss D: 0.0737, loss G: 4.3134\n",
      "Epoch [25/60] Batch 400/1583                   Loss D: 0.0513, loss G: 3.7431\n",
      "Epoch [25/60] Batch 500/1583                   Loss D: 0.0937, loss G: 3.7198\n",
      "Epoch [25/60] Batch 600/1583                   Loss D: 0.0763, loss G: 3.7343\n",
      "Epoch [25/60] Batch 700/1583                   Loss D: 0.1384, loss G: 4.8472\n",
      "Epoch [25/60] Batch 800/1583                   Loss D: 0.1067, loss G: 3.7796\n",
      "Epoch [25/60] Batch 900/1583                   Loss D: 0.1286, loss G: 3.3044\n",
      "Epoch [25/60] Batch 1000/1583                   Loss D: 0.2624, loss G: 2.0099\n",
      "Epoch [25/60] Batch 1100/1583                   Loss D: 0.0885, loss G: 2.9789\n",
      "Epoch [25/60] Batch 1200/1583                   Loss D: 0.5060, loss G: 0.9118\n",
      "Epoch [25/60] Batch 1300/1583                   Loss D: 0.1249, loss G: 4.4845\n",
      "Epoch [25/60] Batch 1400/1583                   Loss D: 0.0774, loss G: 4.0465\n",
      "Epoch [25/60] Batch 1500/1583                   Loss D: 0.5655, loss G: 1.3169\n",
      "Epoch [26/60] Batch 0/1583                   Loss D: 0.1319, loss G: 3.4562\n",
      "Epoch [26/60] Batch 100/1583                   Loss D: 0.5067, loss G: 1.3035\n",
      "Epoch [26/60] Batch 200/1583                   Loss D: 0.0410, loss G: 4.3390\n",
      "Epoch [26/60] Batch 300/1583                   Loss D: 0.0516, loss G: 3.6565\n",
      "Epoch [26/60] Batch 400/1583                   Loss D: 0.1037, loss G: 4.9815\n",
      "Epoch [26/60] Batch 500/1583                   Loss D: 0.1264, loss G: 3.0794\n",
      "Epoch [26/60] Batch 600/1583                   Loss D: 0.1535, loss G: 3.5783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/60] Batch 700/1583                   Loss D: 0.1031, loss G: 4.3485\n",
      "Epoch [26/60] Batch 800/1583                   Loss D: 0.1591, loss G: 4.1837\n",
      "Epoch [26/60] Batch 900/1583                   Loss D: 0.1233, loss G: 5.2720\n",
      "Epoch [26/60] Batch 1000/1583                   Loss D: 0.0762, loss G: 4.7056\n",
      "Epoch [26/60] Batch 1100/1583                   Loss D: 0.0627, loss G: 3.4586\n",
      "Epoch [26/60] Batch 1200/1583                   Loss D: 0.0708, loss G: 3.8858\n",
      "Epoch [26/60] Batch 1300/1583                   Loss D: 0.1294, loss G: 3.0261\n",
      "Epoch [26/60] Batch 1400/1583                   Loss D: 0.0609, loss G: 4.2486\n",
      "Epoch [26/60] Batch 1500/1583                   Loss D: 0.3488, loss G: 6.3006\n",
      "Epoch [27/60] Batch 0/1583                   Loss D: 0.2771, loss G: 2.7149\n",
      "Epoch [27/60] Batch 100/1583                   Loss D: 0.0674, loss G: 3.7253\n",
      "Epoch [27/60] Batch 200/1583                   Loss D: 0.1135, loss G: 4.0328\n",
      "Epoch [27/60] Batch 300/1583                   Loss D: 0.0975, loss G: 3.4360\n",
      "Epoch [27/60] Batch 400/1583                   Loss D: 0.0804, loss G: 3.1673\n",
      "Epoch [27/60] Batch 500/1583                   Loss D: 0.4469, loss G: 4.0261\n",
      "Epoch [27/60] Batch 600/1583                   Loss D: 0.1293, loss G: 3.7796\n",
      "Epoch [27/60] Batch 700/1583                   Loss D: 0.0834, loss G: 4.2637\n",
      "Epoch [27/60] Batch 800/1583                   Loss D: 0.0431, loss G: 4.7023\n",
      "Epoch [27/60] Batch 900/1583                   Loss D: 0.1382, loss G: 3.6604\n",
      "Epoch [27/60] Batch 1000/1583                   Loss D: 0.1046, loss G: 3.7303\n",
      "Epoch [27/60] Batch 1100/1583                   Loss D: 0.1416, loss G: 3.1817\n",
      "Epoch [27/60] Batch 1200/1583                   Loss D: 0.1295, loss G: 3.2186\n",
      "Epoch [27/60] Batch 1300/1583                   Loss D: 0.0923, loss G: 3.7805\n",
      "Epoch [27/60] Batch 1400/1583                   Loss D: 0.1454, loss G: 5.2598\n",
      "Epoch [27/60] Batch 1500/1583                   Loss D: 0.0366, loss G: 5.0064\n",
      "Epoch [28/60] Batch 0/1583                   Loss D: 0.0976, loss G: 4.3316\n",
      "Epoch [28/60] Batch 100/1583                   Loss D: 0.0723, loss G: 3.1105\n",
      "Epoch [28/60] Batch 200/1583                   Loss D: 0.5528, loss G: 8.1247\n",
      "Epoch [28/60] Batch 300/1583                   Loss D: 0.4569, loss G: 2.7870\n",
      "Epoch [28/60] Batch 400/1583                   Loss D: 0.1049, loss G: 3.8157\n",
      "Epoch [28/60] Batch 500/1583                   Loss D: 0.0449, loss G: 4.7120\n",
      "Epoch [28/60] Batch 600/1583                   Loss D: 0.2293, loss G: 0.6239\n",
      "Epoch [28/60] Batch 700/1583                   Loss D: 0.3289, loss G: 3.2321\n",
      "Epoch [28/60] Batch 800/1583                   Loss D: 0.0558, loss G: 4.2634\n",
      "Epoch [28/60] Batch 900/1583                   Loss D: 0.0891, loss G: 3.9860\n",
      "Epoch [28/60] Batch 1000/1583                   Loss D: 0.0877, loss G: 3.6280\n",
      "Epoch [28/60] Batch 1100/1583                   Loss D: 0.1387, loss G: 3.7954\n",
      "Epoch [28/60] Batch 1200/1583                   Loss D: 0.1070, loss G: 4.7969\n",
      "Epoch [28/60] Batch 1300/1583                   Loss D: 0.0446, loss G: 4.4191\n",
      "Epoch [28/60] Batch 1400/1583                   Loss D: 1.2446, loss G: 6.8320\n",
      "Epoch [28/60] Batch 1500/1583                   Loss D: 0.3851, loss G: 1.3024\n",
      "Epoch [29/60] Batch 0/1583                   Loss D: 0.1213, loss G: 2.5345\n",
      "Epoch [29/60] Batch 100/1583                   Loss D: 0.0628, loss G: 4.1783\n",
      "Epoch [29/60] Batch 200/1583                   Loss D: 0.0553, loss G: 3.7760\n",
      "Epoch [29/60] Batch 300/1583                   Loss D: 0.0444, loss G: 4.5509\n",
      "Epoch [29/60] Batch 400/1583                   Loss D: 0.0776, loss G: 3.8231\n",
      "Epoch [29/60] Batch 500/1583                   Loss D: 0.5531, loss G: 1.6150\n",
      "Epoch [29/60] Batch 600/1583                   Loss D: 0.0907, loss G: 4.7157\n",
      "Epoch [29/60] Batch 700/1583                   Loss D: 0.0978, loss G: 4.7173\n",
      "Epoch [29/60] Batch 800/1583                   Loss D: 0.1864, loss G: 4.3682\n",
      "Epoch [29/60] Batch 900/1583                   Loss D: 0.0588, loss G: 4.1241\n",
      "Epoch [29/60] Batch 1000/1583                   Loss D: 0.0419, loss G: 5.0253\n",
      "Epoch [29/60] Batch 1100/1583                   Loss D: 0.4369, loss G: 1.4758\n",
      "Epoch [29/60] Batch 1200/1583                   Loss D: 0.3153, loss G: 1.8539\n",
      "Epoch [29/60] Batch 1300/1583                   Loss D: 0.0657, loss G: 3.8090\n",
      "Epoch [29/60] Batch 1400/1583                   Loss D: 0.0699, loss G: 3.7368\n",
      "Epoch [29/60] Batch 1500/1583                   Loss D: 0.7080, loss G: 6.8048\n",
      "Epoch [30/60] Batch 0/1583                   Loss D: 0.0608, loss G: 4.7056\n",
      "Epoch [30/60] Batch 100/1583                   Loss D: 0.0497, loss G: 3.9137\n",
      "Epoch [30/60] Batch 200/1583                   Loss D: 0.0395, loss G: 4.4920\n",
      "Epoch [30/60] Batch 300/1583                   Loss D: 0.1284, loss G: 3.0272\n",
      "Epoch [30/60] Batch 400/1583                   Loss D: 0.0551, loss G: 4.3908\n",
      "Epoch [30/60] Batch 500/1583                   Loss D: 0.4631, loss G: 1.9910\n",
      "Epoch [30/60] Batch 600/1583                   Loss D: 0.0797, loss G: 4.1358\n",
      "Epoch [30/60] Batch 700/1583                   Loss D: 0.0876, loss G: 4.2927\n",
      "Epoch [30/60] Batch 800/1583                   Loss D: 0.0618, loss G: 3.6874\n",
      "Epoch [30/60] Batch 900/1583                   Loss D: 0.0889, loss G: 3.0553\n",
      "Epoch [30/60] Batch 1000/1583                   Loss D: 0.0905, loss G: 3.4044\n",
      "Epoch [30/60] Batch 1100/1583                   Loss D: 0.1857, loss G: 4.7328\n",
      "Epoch [30/60] Batch 1200/1583                   Loss D: 0.9464, loss G: 9.2473\n",
      "Epoch [30/60] Batch 1300/1583                   Loss D: 0.0636, loss G: 3.8910\n",
      "Epoch [30/60] Batch 1400/1583                   Loss D: 0.0776, loss G: 5.3560\n",
      "Epoch [30/60] Batch 1500/1583                   Loss D: 0.0699, loss G: 3.3530\n",
      "Epoch [31/60] Batch 0/1583                   Loss D: 0.1106, loss G: 3.1570\n",
      "Epoch [31/60] Batch 100/1583                   Loss D: 0.0654, loss G: 4.4197\n",
      "Epoch [31/60] Batch 200/1583                   Loss D: 0.0833, loss G: 3.2799\n",
      "Epoch [31/60] Batch 300/1583                   Loss D: 0.0653, loss G: 4.2764\n",
      "Epoch [31/60] Batch 400/1583                   Loss D: 0.0455, loss G: 4.3044\n",
      "Epoch [31/60] Batch 500/1583                   Loss D: 0.0287, loss G: 4.5368\n",
      "Epoch [31/60] Batch 600/1583                   Loss D: 0.8441, loss G: 0.8225\n",
      "Epoch [31/60] Batch 700/1583                   Loss D: 0.0638, loss G: 3.5263\n",
      "Epoch [31/60] Batch 800/1583                   Loss D: 0.0841, loss G: 5.3279\n",
      "Epoch [31/60] Batch 900/1583                   Loss D: 0.0564, loss G: 4.2912\n",
      "Epoch [31/60] Batch 1000/1583                   Loss D: 0.1339, loss G: 3.3479\n",
      "Epoch [31/60] Batch 1100/1583                   Loss D: 0.1104, loss G: 2.9141\n",
      "Epoch [31/60] Batch 1200/1583                   Loss D: 0.0720, loss G: 3.2624\n",
      "Epoch [31/60] Batch 1300/1583                   Loss D: 0.1727, loss G: 3.5228\n",
      "Epoch [31/60] Batch 1400/1583                   Loss D: 0.0964, loss G: 5.1663\n",
      "Epoch [31/60] Batch 1500/1583                   Loss D: 0.1452, loss G: 4.9889\n",
      "Epoch [32/60] Batch 0/1583                   Loss D: 0.2184, loss G: 3.9689\n",
      "Epoch [32/60] Batch 100/1583                   Loss D: 0.0651, loss G: 3.2162\n",
      "Epoch [32/60] Batch 200/1583                   Loss D: 0.0293, loss G: 3.6472\n",
      "Epoch [32/60] Batch 300/1583                   Loss D: 0.0448, loss G: 4.5853\n",
      "Epoch [32/60] Batch 400/1583                   Loss D: 0.0338, loss G: 4.4245\n",
      "Epoch [32/60] Batch 500/1583                   Loss D: 0.0733, loss G: 4.2739\n",
      "Epoch [32/60] Batch 600/1583                   Loss D: 0.0665, loss G: 3.5476\n",
      "Epoch [32/60] Batch 700/1583                   Loss D: 0.0839, loss G: 4.7993\n",
      "Epoch [32/60] Batch 800/1583                   Loss D: 0.0519, loss G: 4.4748\n",
      "Epoch [32/60] Batch 900/1583                   Loss D: 0.0933, loss G: 4.2484\n",
      "Epoch [32/60] Batch 1000/1583                   Loss D: 0.0705, loss G: 4.5987\n",
      "Epoch [32/60] Batch 1100/1583                   Loss D: 0.1300, loss G: 6.3248\n",
      "Epoch [32/60] Batch 1200/1583                   Loss D: 0.1610, loss G: 6.6703\n",
      "Epoch [32/60] Batch 1300/1583                   Loss D: 0.2211, loss G: 2.2166\n",
      "Epoch [32/60] Batch 1400/1583                   Loss D: 0.0595, loss G: 4.5520\n",
      "Epoch [32/60] Batch 1500/1583                   Loss D: 0.0301, loss G: 4.1908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/60] Batch 0/1583                   Loss D: 0.3394, loss G: 1.6158\n",
      "Epoch [33/60] Batch 100/1583                   Loss D: 0.3819, loss G: 0.9508\n",
      "Epoch [33/60] Batch 200/1583                   Loss D: 0.3004, loss G: 9.0620\n",
      "Epoch [33/60] Batch 300/1583                   Loss D: 0.0224, loss G: 4.4509\n",
      "Epoch [33/60] Batch 400/1583                   Loss D: 0.1456, loss G: 3.2879\n",
      "Epoch [33/60] Batch 500/1583                   Loss D: 0.0321, loss G: 5.0528\n",
      "Epoch [33/60] Batch 600/1583                   Loss D: 0.0660, loss G: 4.3117\n",
      "Epoch [33/60] Batch 700/1583                   Loss D: 0.0606, loss G: 3.4931\n",
      "Epoch [33/60] Batch 800/1583                   Loss D: 0.0526, loss G: 3.4857\n",
      "Epoch [33/60] Batch 900/1583                   Loss D: 0.0888, loss G: 5.8551\n",
      "Epoch [33/60] Batch 1000/1583                   Loss D: 0.0477, loss G: 3.1534\n",
      "Epoch [33/60] Batch 1100/1583                   Loss D: 0.1393, loss G: 2.7919\n",
      "Epoch [33/60] Batch 1200/1583                   Loss D: 0.0702, loss G: 3.4526\n",
      "Epoch [33/60] Batch 1300/1583                   Loss D: 0.0524, loss G: 4.0069\n",
      "Epoch [33/60] Batch 1400/1583                   Loss D: 0.7918, loss G: 0.3419\n",
      "Epoch [33/60] Batch 1500/1583                   Loss D: 0.0725, loss G: 3.7972\n",
      "Epoch [34/60] Batch 0/1583                   Loss D: 0.0829, loss G: 4.6357\n",
      "Epoch [34/60] Batch 100/1583                   Loss D: 0.0391, loss G: 5.0072\n",
      "Epoch [34/60] Batch 200/1583                   Loss D: 0.0377, loss G: 3.7697\n",
      "Epoch [34/60] Batch 300/1583                   Loss D: 0.0495, loss G: 4.6588\n",
      "Epoch [34/60] Batch 400/1583                   Loss D: 0.0473, loss G: 3.6771\n",
      "Epoch [34/60] Batch 500/1583                   Loss D: 0.1229, loss G: 3.8999\n",
      "Epoch [34/60] Batch 600/1583                   Loss D: 0.0354, loss G: 5.4981\n",
      "Epoch [34/60] Batch 700/1583                   Loss D: 0.0426, loss G: 5.3370\n",
      "Epoch [34/60] Batch 800/1583                   Loss D: 0.0719, loss G: 5.8925\n",
      "Epoch [34/60] Batch 900/1583                   Loss D: 0.0893, loss G: 3.0306\n",
      "Epoch [34/60] Batch 1000/1583                   Loss D: 0.1046, loss G: 4.3030\n",
      "Epoch [34/60] Batch 1100/1583                   Loss D: 0.1197, loss G: 5.4794\n",
      "Epoch [34/60] Batch 1200/1583                   Loss D: 0.0950, loss G: 4.6806\n",
      "Epoch [34/60] Batch 1300/1583                   Loss D: 0.2319, loss G: 2.2153\n",
      "Epoch [34/60] Batch 1400/1583                   Loss D: 0.2873, loss G: 3.9997\n",
      "Epoch [34/60] Batch 1500/1583                   Loss D: 0.0875, loss G: 3.5468\n",
      "Epoch [35/60] Batch 0/1583                   Loss D: 0.0379, loss G: 4.6526\n",
      "Epoch [35/60] Batch 100/1583                   Loss D: 0.2203, loss G: 4.1441\n",
      "Epoch [35/60] Batch 200/1583                   Loss D: 0.0572, loss G: 5.1602\n",
      "Epoch [35/60] Batch 300/1583                   Loss D: 0.1402, loss G: 4.0067\n",
      "Epoch [35/60] Batch 400/1583                   Loss D: 0.0920, loss G: 4.6868\n",
      "Epoch [35/60] Batch 500/1583                   Loss D: 0.0366, loss G: 5.2483\n",
      "Epoch [35/60] Batch 600/1583                   Loss D: 0.1282, loss G: 2.8348\n",
      "Epoch [35/60] Batch 700/1583                   Loss D: 0.0415, loss G: 4.1742\n",
      "Epoch [35/60] Batch 800/1583                   Loss D: 0.1581, loss G: 4.5506\n",
      "Epoch [35/60] Batch 900/1583                   Loss D: 0.9260, loss G: 1.0626\n",
      "Epoch [35/60] Batch 1000/1583                   Loss D: 0.0691, loss G: 5.4938\n",
      "Epoch [35/60] Batch 1100/1583                   Loss D: 0.0449, loss G: 3.7437\n",
      "Epoch [35/60] Batch 1200/1583                   Loss D: 0.4223, loss G: 3.1715\n",
      "Epoch [35/60] Batch 1300/1583                   Loss D: 0.0493, loss G: 3.8330\n",
      "Epoch [35/60] Batch 1400/1583                   Loss D: 0.0503, loss G: 4.4520\n",
      "Epoch [35/60] Batch 1500/1583                   Loss D: 0.0440, loss G: 5.9497\n",
      "Epoch [36/60] Batch 0/1583                   Loss D: 0.0548, loss G: 4.2287\n",
      "Epoch [36/60] Batch 100/1583                   Loss D: 0.1275, loss G: 6.1917\n",
      "Epoch [36/60] Batch 200/1583                   Loss D: 0.0452, loss G: 5.0482\n",
      "Epoch [36/60] Batch 300/1583                   Loss D: 0.0167, loss G: 4.6671\n",
      "Epoch [36/60] Batch 400/1583                   Loss D: 0.1426, loss G: 2.1699\n",
      "Epoch [36/60] Batch 500/1583                   Loss D: 0.0730, loss G: 4.6174\n",
      "Epoch [36/60] Batch 600/1583                   Loss D: 0.3939, loss G: 2.9665\n",
      "Epoch [36/60] Batch 700/1583                   Loss D: 0.0412, loss G: 5.2658\n",
      "Epoch [36/60] Batch 800/1583                   Loss D: 0.0385, loss G: 4.3009\n",
      "Epoch [36/60] Batch 900/1583                   Loss D: 0.0174, loss G: 6.4393\n",
      "Epoch [36/60] Batch 1000/1583                   Loss D: 0.1013, loss G: 4.2574\n",
      "Epoch [36/60] Batch 1100/1583                   Loss D: 0.2298, loss G: 3.0576\n",
      "Epoch [36/60] Batch 1200/1583                   Loss D: 0.0951, loss G: 3.2528\n",
      "Epoch [36/60] Batch 1300/1583                   Loss D: 0.0557, loss G: 3.7710\n",
      "Epoch [36/60] Batch 1400/1583                   Loss D: 0.0450, loss G: 4.6127\n",
      "Epoch [36/60] Batch 1500/1583                   Loss D: 0.0379, loss G: 4.5709\n",
      "Epoch [37/60] Batch 0/1583                   Loss D: 0.0643, loss G: 4.7533\n",
      "Epoch [37/60] Batch 100/1583                   Loss D: 0.0447, loss G: 5.0356\n",
      "Epoch [37/60] Batch 200/1583                   Loss D: 0.0274, loss G: 4.5779\n",
      "Epoch [37/60] Batch 300/1583                   Loss D: 0.2260, loss G: 4.1521\n",
      "Epoch [37/60] Batch 400/1583                   Loss D: 0.0602, loss G: 5.8893\n",
      "Epoch [37/60] Batch 500/1583                   Loss D: 0.0398, loss G: 4.9972\n",
      "Epoch [37/60] Batch 600/1583                   Loss D: 0.1758, loss G: 1.3479\n",
      "Epoch [37/60] Batch 700/1583                   Loss D: 0.0431, loss G: 4.6211\n",
      "Epoch [37/60] Batch 800/1583                   Loss D: 0.3988, loss G: 7.3438\n",
      "Epoch [37/60] Batch 900/1583                   Loss D: 0.0368, loss G: 4.4369\n",
      "Epoch [37/60] Batch 1000/1583                   Loss D: 0.1597, loss G: 6.2724\n",
      "Epoch [37/60] Batch 1100/1583                   Loss D: 0.1518, loss G: 4.7712\n",
      "Epoch [37/60] Batch 1200/1583                   Loss D: 0.1211, loss G: 5.1055\n",
      "Epoch [37/60] Batch 1300/1583                   Loss D: 0.0766, loss G: 4.2428\n",
      "Epoch [37/60] Batch 1400/1583                   Loss D: 0.0332, loss G: 4.1157\n",
      "Epoch [37/60] Batch 1500/1583                   Loss D: 0.1069, loss G: 5.0110\n",
      "Epoch [38/60] Batch 0/1583                   Loss D: 0.0784, loss G: 3.6188\n",
      "Epoch [38/60] Batch 100/1583                   Loss D: 0.0734, loss G: 3.2572\n",
      "Epoch [38/60] Batch 200/1583                   Loss D: 0.0347, loss G: 4.7150\n",
      "Epoch [38/60] Batch 300/1583                   Loss D: 0.0338, loss G: 4.7791\n",
      "Epoch [38/60] Batch 400/1583                   Loss D: 0.0408, loss G: 4.8455\n",
      "Epoch [38/60] Batch 500/1583                   Loss D: 0.0410, loss G: 4.7651\n",
      "Epoch [38/60] Batch 600/1583                   Loss D: 0.3212, loss G: 2.3516\n",
      "Epoch [38/60] Batch 700/1583                   Loss D: 0.0416, loss G: 4.5544\n",
      "Epoch [38/60] Batch 800/1583                   Loss D: 0.0695, loss G: 4.8374\n",
      "Epoch [38/60] Batch 900/1583                   Loss D: 0.0595, loss G: 5.1297\n",
      "Epoch [38/60] Batch 1000/1583                   Loss D: 0.1962, loss G: 2.6985\n",
      "Epoch [38/60] Batch 1100/1583                   Loss D: 0.0545, loss G: 3.9047\n",
      "Epoch [38/60] Batch 1200/1583                   Loss D: 0.0665, loss G: 5.7881\n",
      "Epoch [38/60] Batch 1300/1583                   Loss D: 0.0425, loss G: 4.0960\n",
      "Epoch [38/60] Batch 1400/1583                   Loss D: 0.0389, loss G: 4.7368\n",
      "Epoch [38/60] Batch 1500/1583                   Loss D: 0.1069, loss G: 4.2913\n",
      "Epoch [39/60] Batch 0/1583                   Loss D: 0.0374, loss G: 4.4344\n",
      "Epoch [39/60] Batch 100/1583                   Loss D: 0.1857, loss G: 2.6694\n",
      "Epoch [39/60] Batch 200/1583                   Loss D: 0.0451, loss G: 4.5784\n",
      "Epoch [39/60] Batch 300/1583                   Loss D: 0.1937, loss G: 7.6197\n",
      "Epoch [39/60] Batch 400/1583                   Loss D: 0.0260, loss G: 4.4766\n",
      "Epoch [39/60] Batch 500/1583                   Loss D: 0.0831, loss G: 3.3978\n",
      "Epoch [39/60] Batch 600/1583                   Loss D: 0.0284, loss G: 5.2652\n",
      "Epoch [39/60] Batch 700/1583                   Loss D: 0.0512, loss G: 5.1680\n",
      "Epoch [39/60] Batch 800/1583                   Loss D: 0.1092, loss G: 3.8126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/60] Batch 900/1583                   Loss D: 0.0615, loss G: 4.7838\n",
      "Epoch [39/60] Batch 1000/1583                   Loss D: 0.0463, loss G: 4.7987\n",
      "Epoch [39/60] Batch 1100/1583                   Loss D: 0.1040, loss G: 5.8268\n",
      "Epoch [39/60] Batch 1200/1583                   Loss D: 0.0694, loss G: 4.9897\n",
      "Epoch [39/60] Batch 1300/1583                   Loss D: 0.0414, loss G: 4.8090\n",
      "Epoch [39/60] Batch 1400/1583                   Loss D: 0.0638, loss G: 4.9757\n",
      "Epoch [39/60] Batch 1500/1583                   Loss D: 0.3727, loss G: 1.7110\n",
      "Epoch [40/60] Batch 0/1583                   Loss D: 0.1674, loss G: 5.3018\n",
      "Epoch [40/60] Batch 100/1583                   Loss D: 0.1229, loss G: 6.2356\n",
      "Epoch [40/60] Batch 200/1583                   Loss D: 0.0597, loss G: 4.1685\n",
      "Epoch [40/60] Batch 300/1583                   Loss D: 0.0450, loss G: 4.8095\n",
      "Epoch [40/60] Batch 400/1583                   Loss D: 0.1307, loss G: 4.4967\n",
      "Epoch [40/60] Batch 500/1583                   Loss D: 0.0807, loss G: 6.4926\n",
      "Epoch [40/60] Batch 600/1583                   Loss D: 0.0329, loss G: 4.6462\n",
      "Epoch [40/60] Batch 700/1583                   Loss D: 0.0121, loss G: 5.9068\n",
      "Epoch [40/60] Batch 800/1583                   Loss D: 0.0759, loss G: 4.3286\n",
      "Epoch [40/60] Batch 900/1583                   Loss D: 0.0576, loss G: 4.6270\n",
      "Epoch [40/60] Batch 1000/1583                   Loss D: 0.0424, loss G: 4.8195\n",
      "Epoch [40/60] Batch 1100/1583                   Loss D: 0.0360, loss G: 4.7510\n",
      "Epoch [40/60] Batch 1200/1583                   Loss D: 0.0521, loss G: 4.4236\n",
      "Epoch [40/60] Batch 1300/1583                   Loss D: 0.2140, loss G: 3.1233\n",
      "Epoch [40/60] Batch 1400/1583                   Loss D: 0.0572, loss G: 5.5223\n",
      "Epoch [40/60] Batch 1500/1583                   Loss D: 0.0312, loss G: 4.4773\n",
      "Epoch [41/60] Batch 0/1583                   Loss D: 0.0614, loss G: 5.0976\n",
      "Epoch [41/60] Batch 100/1583                   Loss D: 0.0923, loss G: 4.6248\n",
      "Epoch [41/60] Batch 200/1583                   Loss D: 0.0684, loss G: 5.7594\n",
      "Epoch [41/60] Batch 300/1583                   Loss D: 0.0297, loss G: 3.4718\n",
      "Epoch [41/60] Batch 400/1583                   Loss D: 0.1444, loss G: 2.1988\n",
      "Epoch [41/60] Batch 500/1583                   Loss D: 0.0364, loss G: 5.2900\n",
      "Epoch [41/60] Batch 600/1583                   Loss D: 0.0654, loss G: 5.0016\n",
      "Epoch [41/60] Batch 700/1583                   Loss D: 0.3982, loss G: 1.9824\n",
      "Epoch [41/60] Batch 800/1583                   Loss D: 0.1103, loss G: 2.0862\n",
      "Epoch [41/60] Batch 900/1583                   Loss D: 0.0488, loss G: 6.6457\n",
      "Epoch [41/60] Batch 1000/1583                   Loss D: 0.0421, loss G: 4.4895\n",
      "Epoch [41/60] Batch 1100/1583                   Loss D: 0.0302, loss G: 5.3897\n",
      "Epoch [41/60] Batch 1200/1583                   Loss D: 0.0375, loss G: 4.2984\n",
      "Epoch [41/60] Batch 1300/1583                   Loss D: 0.0407, loss G: 5.1540\n",
      "Epoch [41/60] Batch 1400/1583                   Loss D: 0.0220, loss G: 5.1018\n",
      "Epoch [41/60] Batch 1500/1583                   Loss D: 0.0441, loss G: 4.9567\n",
      "Epoch [42/60] Batch 0/1583                   Loss D: 0.0498, loss G: 6.1227\n",
      "Epoch [42/60] Batch 100/1583                   Loss D: 0.0204, loss G: 4.7193\n",
      "Epoch [42/60] Batch 200/1583                   Loss D: 0.2246, loss G: 3.8412\n",
      "Epoch [42/60] Batch 300/1583                   Loss D: 0.0372, loss G: 4.9193\n",
      "Epoch [42/60] Batch 400/1583                   Loss D: 0.0188, loss G: 5.3476\n",
      "Epoch [42/60] Batch 500/1583                   Loss D: 0.0408, loss G: 5.2930\n",
      "Epoch [42/60] Batch 600/1583                   Loss D: 0.0094, loss G: 6.8807\n",
      "Epoch [42/60] Batch 700/1583                   Loss D: 0.1764, loss G: 5.0642\n",
      "Epoch [42/60] Batch 800/1583                   Loss D: 0.1168, loss G: 5.0946\n",
      "Epoch [42/60] Batch 900/1583                   Loss D: 0.0314, loss G: 4.9564\n",
      "Epoch [42/60] Batch 1000/1583                   Loss D: 0.0412, loss G: 3.6812\n",
      "Epoch [42/60] Batch 1100/1583                   Loss D: 0.0634, loss G: 5.5227\n",
      "Epoch [42/60] Batch 1200/1583                   Loss D: 0.5440, loss G: 8.1094\n",
      "Epoch [42/60] Batch 1300/1583                   Loss D: 0.1358, loss G: 3.3347\n",
      "Epoch [42/60] Batch 1400/1583                   Loss D: 0.0453, loss G: 5.1002\n",
      "Epoch [42/60] Batch 1500/1583                   Loss D: 0.0235, loss G: 5.0882\n",
      "Epoch [43/60] Batch 0/1583                   Loss D: 0.0981, loss G: 4.0410\n",
      "Epoch [43/60] Batch 100/1583                   Loss D: 0.0347, loss G: 4.9707\n",
      "Epoch [43/60] Batch 200/1583                   Loss D: 0.0179, loss G: 5.8553\n",
      "Epoch [43/60] Batch 300/1583                   Loss D: 0.0365, loss G: 5.3719\n",
      "Epoch [43/60] Batch 400/1583                   Loss D: 0.0270, loss G: 4.6373\n",
      "Epoch [43/60] Batch 500/1583                   Loss D: 0.0446, loss G: 4.2462\n",
      "Epoch [43/60] Batch 600/1583                   Loss D: 0.0630, loss G: 5.5026\n",
      "Epoch [43/60] Batch 700/1583                   Loss D: 2.1698, loss G: 0.2665\n",
      "Epoch [43/60] Batch 800/1583                   Loss D: 0.0966, loss G: 4.4950\n",
      "Epoch [43/60] Batch 900/1583                   Loss D: 0.0582, loss G: 4.0064\n",
      "Epoch [43/60] Batch 1000/1583                   Loss D: 0.1358, loss G: 4.8945\n",
      "Epoch [43/60] Batch 1100/1583                   Loss D: 0.0395, loss G: 5.0159\n",
      "Epoch [43/60] Batch 1200/1583                   Loss D: 0.0365, loss G: 4.7168\n",
      "Epoch [43/60] Batch 1300/1583                   Loss D: 0.0761, loss G: 3.3762\n",
      "Epoch [43/60] Batch 1400/1583                   Loss D: 0.0371, loss G: 4.9880\n",
      "Epoch [43/60] Batch 1500/1583                   Loss D: 2.0895, loss G: 9.0418\n",
      "Epoch [44/60] Batch 0/1583                   Loss D: 0.0792, loss G: 4.5007\n",
      "Epoch [44/60] Batch 100/1583                   Loss D: 0.0327, loss G: 4.9459\n",
      "Epoch [44/60] Batch 200/1583                   Loss D: 0.0403, loss G: 3.8753\n",
      "Epoch [44/60] Batch 300/1583                   Loss D: 1.1551, loss G: 0.8425\n",
      "Epoch [44/60] Batch 400/1583                   Loss D: 0.0999, loss G: 3.2072\n",
      "Epoch [44/60] Batch 500/1583                   Loss D: 0.0345, loss G: 4.9894\n",
      "Epoch [44/60] Batch 600/1583                   Loss D: 0.0312, loss G: 4.9500\n",
      "Epoch [44/60] Batch 700/1583                   Loss D: 0.0592, loss G: 5.9608\n",
      "Epoch [44/60] Batch 800/1583                   Loss D: 0.1629, loss G: 3.0345\n",
      "Epoch [44/60] Batch 900/1583                   Loss D: 0.0298, loss G: 4.6646\n",
      "Epoch [44/60] Batch 1000/1583                   Loss D: 0.0195, loss G: 5.0317\n",
      "Epoch [44/60] Batch 1100/1583                   Loss D: 0.0472, loss G: 4.3325\n",
      "Epoch [44/60] Batch 1200/1583                   Loss D: 0.0198, loss G: 4.9859\n",
      "Epoch [44/60] Batch 1300/1583                   Loss D: 0.0245, loss G: 4.9619\n",
      "Epoch [44/60] Batch 1400/1583                   Loss D: 0.4913, loss G: 1.5550\n",
      "Epoch [44/60] Batch 1500/1583                   Loss D: 0.0922, loss G: 3.2383\n",
      "Epoch [45/60] Batch 0/1583                   Loss D: 0.0625, loss G: 4.1314\n",
      "Epoch [45/60] Batch 100/1583                   Loss D: 0.0497, loss G: 3.7394\n",
      "Epoch [45/60] Batch 200/1583                   Loss D: 1.0636, loss G: 7.8097\n",
      "Epoch [45/60] Batch 300/1583                   Loss D: 0.0299, loss G: 5.1664\n",
      "Epoch [45/60] Batch 400/1583                   Loss D: 0.0314, loss G: 4.5565\n",
      "Epoch [45/60] Batch 500/1583                   Loss D: 0.0673, loss G: 4.4883\n",
      "Epoch [45/60] Batch 600/1583                   Loss D: 0.0627, loss G: 5.3194\n",
      "Epoch [45/60] Batch 700/1583                   Loss D: 0.0504, loss G: 4.0714\n",
      "Epoch [45/60] Batch 800/1583                   Loss D: 0.0336, loss G: 6.5888\n",
      "Epoch [45/60] Batch 900/1583                   Loss D: 0.5648, loss G: 11.4662\n",
      "Epoch [45/60] Batch 1000/1583                   Loss D: 0.0485, loss G: 4.9670\n",
      "Epoch [45/60] Batch 1100/1583                   Loss D: 0.0550, loss G: 5.6291\n",
      "Epoch [45/60] Batch 1200/1583                   Loss D: 0.0344, loss G: 5.4979\n",
      "Epoch [45/60] Batch 1300/1583                   Loss D: 0.4015, loss G: 1.8209\n",
      "Epoch [45/60] Batch 1400/1583                   Loss D: 0.0578, loss G: 4.5306\n",
      "Epoch [45/60] Batch 1500/1583                   Loss D: 0.0635, loss G: 6.0528\n",
      "Epoch [46/60] Batch 0/1583                   Loss D: 0.0189, loss G: 5.1430\n",
      "Epoch [46/60] Batch 100/1583                   Loss D: 0.0421, loss G: 4.5630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/60] Batch 200/1583                   Loss D: 0.1096, loss G: 3.7298\n",
      "Epoch [46/60] Batch 300/1583                   Loss D: 0.0395, loss G: 5.0388\n",
      "Epoch [46/60] Batch 400/1583                   Loss D: 0.0312, loss G: 4.6860\n",
      "Epoch [46/60] Batch 500/1583                   Loss D: 0.0606, loss G: 4.3444\n",
      "Epoch [46/60] Batch 600/1583                   Loss D: 0.1120, loss G: 1.9985\n",
      "Epoch [46/60] Batch 700/1583                   Loss D: 0.0256, loss G: 4.7711\n",
      "Epoch [46/60] Batch 800/1583                   Loss D: 0.0881, loss G: 4.3782\n",
      "Epoch [46/60] Batch 900/1583                   Loss D: 0.0856, loss G: 5.8023\n",
      "Epoch [46/60] Batch 1000/1583                   Loss D: 0.0210, loss G: 5.6663\n",
      "Epoch [46/60] Batch 1100/1583                   Loss D: 0.0500, loss G: 4.0793\n",
      "Epoch [46/60] Batch 1200/1583                   Loss D: 0.0310, loss G: 4.7894\n",
      "Epoch [46/60] Batch 1300/1583                   Loss D: 0.0744, loss G: 6.0457\n",
      "Epoch [46/60] Batch 1400/1583                   Loss D: 0.0213, loss G: 4.9740\n",
      "Epoch [46/60] Batch 1500/1583                   Loss D: 0.0121, loss G: 5.7240\n",
      "Epoch [47/60] Batch 0/1583                   Loss D: 0.0368, loss G: 5.4335\n",
      "Epoch [47/60] Batch 100/1583                   Loss D: 0.0500, loss G: 5.0336\n",
      "Epoch [47/60] Batch 200/1583                   Loss D: 0.0343, loss G: 5.4232\n",
      "Epoch [47/60] Batch 300/1583                   Loss D: 0.0441, loss G: 4.6667\n",
      "Epoch [47/60] Batch 400/1583                   Loss D: 0.0206, loss G: 4.4403\n",
      "Epoch [47/60] Batch 500/1583                   Loss D: 0.0554, loss G: 6.7805\n",
      "Epoch [47/60] Batch 600/1583                   Loss D: 0.0130, loss G: 5.5676\n",
      "Epoch [47/60] Batch 700/1583                   Loss D: 0.1621, loss G: 5.4580\n",
      "Epoch [47/60] Batch 800/1583                   Loss D: 0.0197, loss G: 5.3131\n",
      "Epoch [47/60] Batch 900/1583                   Loss D: 0.3027, loss G: 2.4340\n",
      "Epoch [47/60] Batch 1000/1583                   Loss D: 0.1306, loss G: 4.1547\n",
      "Epoch [47/60] Batch 1100/1583                   Loss D: 0.0639, loss G: 4.2210\n",
      "Epoch [47/60] Batch 1200/1583                   Loss D: 0.0300, loss G: 5.3858\n",
      "Epoch [47/60] Batch 1300/1583                   Loss D: 0.0507, loss G: 3.9345\n",
      "Epoch [47/60] Batch 1400/1583                   Loss D: 0.0183, loss G: 5.5660\n",
      "Epoch [47/60] Batch 1500/1583                   Loss D: 0.0174, loss G: 4.7612\n",
      "Epoch [48/60] Batch 0/1583                   Loss D: 0.2507, loss G: 3.1151\n",
      "Epoch [48/60] Batch 100/1583                   Loss D: 0.0419, loss G: 5.1128\n",
      "Epoch [48/60] Batch 200/1583                   Loss D: 0.0337, loss G: 5.3016\n",
      "Epoch [48/60] Batch 300/1583                   Loss D: 0.1676, loss G: 1.8657\n",
      "Epoch [48/60] Batch 400/1583                   Loss D: 0.0331, loss G: 5.0777\n",
      "Epoch [48/60] Batch 500/1583                   Loss D: 0.1426, loss G: 4.9688\n",
      "Epoch [48/60] Batch 600/1583                   Loss D: 0.0194, loss G: 5.7652\n",
      "Epoch [48/60] Batch 700/1583                   Loss D: 0.0260, loss G: 5.2367\n",
      "Epoch [48/60] Batch 800/1583                   Loss D: 0.0297, loss G: 4.4583\n",
      "Epoch [48/60] Batch 900/1583                   Loss D: 0.0332, loss G: 5.1209\n",
      "Epoch [48/60] Batch 1000/1583                   Loss D: 0.0342, loss G: 6.0209\n",
      "Epoch [48/60] Batch 1100/1583                   Loss D: 0.0225, loss G: 5.3768\n",
      "Epoch [48/60] Batch 1200/1583                   Loss D: 0.1054, loss G: 4.2678\n",
      "Epoch [48/60] Batch 1300/1583                   Loss D: 0.0464, loss G: 4.3483\n",
      "Epoch [48/60] Batch 1400/1583                   Loss D: 0.0310, loss G: 5.1347\n",
      "Epoch [48/60] Batch 1500/1583                   Loss D: 0.0914, loss G: 4.4756\n",
      "Epoch [49/60] Batch 0/1583                   Loss D: 0.0519, loss G: 2.9793\n",
      "Epoch [49/60] Batch 100/1583                   Loss D: 0.0218, loss G: 4.7460\n",
      "Epoch [49/60] Batch 200/1583                   Loss D: 0.0974, loss G: 6.8354\n",
      "Epoch [49/60] Batch 300/1583                   Loss D: 0.0269, loss G: 4.9501\n",
      "Epoch [49/60] Batch 400/1583                   Loss D: 0.0222, loss G: 4.8336\n",
      "Epoch [49/60] Batch 500/1583                   Loss D: 0.0507, loss G: 3.8243\n",
      "Epoch [49/60] Batch 600/1583                   Loss D: 0.2569, loss G: 0.1881\n",
      "Epoch [49/60] Batch 700/1583                   Loss D: 0.0225, loss G: 4.9292\n",
      "Epoch [49/60] Batch 800/1583                   Loss D: 0.0227, loss G: 5.5478\n",
      "Epoch [49/60] Batch 900/1583                   Loss D: 0.0155, loss G: 5.8257\n",
      "Epoch [49/60] Batch 1000/1583                   Loss D: 0.0256, loss G: 5.6196\n",
      "Epoch [49/60] Batch 1100/1583                   Loss D: 3.1855, loss G: 0.2534\n",
      "Epoch [49/60] Batch 1200/1583                   Loss D: 0.0443, loss G: 4.9724\n",
      "Epoch [49/60] Batch 1300/1583                   Loss D: 0.0530, loss G: 3.9976\n",
      "Epoch [49/60] Batch 1400/1583                   Loss D: 0.0792, loss G: 4.8935\n",
      "Epoch [49/60] Batch 1500/1583                   Loss D: 0.2306, loss G: 3.7332\n",
      "Epoch [50/60] Batch 0/1583                   Loss D: 0.0450, loss G: 5.9024\n",
      "Epoch [50/60] Batch 100/1583                   Loss D: 0.0249, loss G: 5.4108\n",
      "Epoch [50/60] Batch 200/1583                   Loss D: 0.0243, loss G: 4.8767\n",
      "Epoch [50/60] Batch 300/1583                   Loss D: 0.0111, loss G: 5.5644\n",
      "Epoch [50/60] Batch 400/1583                   Loss D: 0.0705, loss G: 4.1962\n",
      "Epoch [50/60] Batch 500/1583                   Loss D: 0.0369, loss G: 5.4284\n",
      "Epoch [50/60] Batch 600/1583                   Loss D: 0.4571, loss G: 3.0514\n",
      "Epoch [50/60] Batch 700/1583                   Loss D: 0.6047, loss G: 14.1133\n",
      "Epoch [50/60] Batch 800/1583                   Loss D: 0.0236, loss G: 4.9252\n",
      "Epoch [50/60] Batch 900/1583                   Loss D: 0.0265, loss G: 7.8253\n",
      "Epoch [50/60] Batch 1000/1583                   Loss D: 0.0233, loss G: 4.3172\n",
      "Epoch [50/60] Batch 1100/1583                   Loss D: 0.0336, loss G: 5.1903\n",
      "Epoch [50/60] Batch 1200/1583                   Loss D: 0.0540, loss G: 6.8836\n",
      "Epoch [50/60] Batch 1300/1583                   Loss D: 0.0590, loss G: 6.2959\n",
      "Epoch [50/60] Batch 1400/1583                   Loss D: 0.0825, loss G: 3.8688\n",
      "Epoch [50/60] Batch 1500/1583                   Loss D: 0.0575, loss G: 5.0197\n",
      "Epoch [51/60] Batch 0/1583                   Loss D: 0.0386, loss G: 4.2061\n",
      "Epoch [51/60] Batch 100/1583                   Loss D: 0.0190, loss G: 4.6693\n",
      "Epoch [51/60] Batch 200/1583                   Loss D: 0.0113, loss G: 5.5351\n",
      "Epoch [51/60] Batch 300/1583                   Loss D: 0.0202, loss G: 6.1109\n",
      "Epoch [51/60] Batch 400/1583                   Loss D: 0.0421, loss G: 4.9997\n",
      "Epoch [51/60] Batch 500/1583                   Loss D: 0.0489, loss G: 3.9270\n",
      "Epoch [51/60] Batch 600/1583                   Loss D: 0.0662, loss G: 5.2694\n",
      "Epoch [51/60] Batch 700/1583                   Loss D: 0.5125, loss G: 2.0253\n",
      "Epoch [51/60] Batch 800/1583                   Loss D: 0.0322, loss G: 4.6531\n",
      "Epoch [51/60] Batch 900/1583                   Loss D: 0.0309, loss G: 5.6344\n",
      "Epoch [51/60] Batch 1000/1583                   Loss D: 0.0344, loss G: 5.2488\n",
      "Epoch [51/60] Batch 1100/1583                   Loss D: 0.0834, loss G: 5.7346\n",
      "Epoch [51/60] Batch 1200/1583                   Loss D: 0.0787, loss G: 4.2856\n",
      "Epoch [51/60] Batch 1300/1583                   Loss D: 0.0650, loss G: 4.3409\n",
      "Epoch [51/60] Batch 1400/1583                   Loss D: 0.0371, loss G: 4.7305\n",
      "Epoch [51/60] Batch 1500/1583                   Loss D: 0.0555, loss G: 4.5497\n",
      "Epoch [52/60] Batch 0/1583                   Loss D: 0.0359, loss G: 4.2837\n",
      "Epoch [52/60] Batch 100/1583                   Loss D: 0.1718, loss G: 3.6198\n",
      "Epoch [52/60] Batch 200/1583                   Loss D: 0.0165, loss G: 7.1529\n",
      "Epoch [52/60] Batch 300/1583                   Loss D: 0.0230, loss G: 5.9182\n",
      "Epoch [52/60] Batch 400/1583                   Loss D: 0.0394, loss G: 4.3316\n",
      "Epoch [52/60] Batch 500/1583                   Loss D: 0.0365, loss G: 4.6883\n",
      "Epoch [52/60] Batch 600/1583                   Loss D: 0.0548, loss G: 3.7111\n",
      "Epoch [52/60] Batch 700/1583                   Loss D: 0.0519, loss G: 5.1569\n",
      "Epoch [52/60] Batch 800/1583                   Loss D: 0.2862, loss G: 11.0988\n",
      "Epoch [52/60] Batch 900/1583                   Loss D: 0.1940, loss G: 3.1184\n",
      "Epoch [52/60] Batch 1000/1583                   Loss D: 0.0414, loss G: 4.6189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/60] Batch 1100/1583                   Loss D: 0.0174, loss G: 5.2123\n",
      "Epoch [52/60] Batch 1200/1583                   Loss D: 0.0648, loss G: 4.4279\n",
      "Epoch [52/60] Batch 1300/1583                   Loss D: 0.0309, loss G: 5.2249\n",
      "Epoch [52/60] Batch 1400/1583                   Loss D: 0.0518, loss G: 4.2508\n",
      "Epoch [52/60] Batch 1500/1583                   Loss D: 0.0397, loss G: 5.7068\n",
      "Epoch [53/60] Batch 0/1583                   Loss D: 0.0239, loss G: 5.1803\n",
      "Epoch [53/60] Batch 100/1583                   Loss D: 0.0166, loss G: 5.8034\n",
      "Epoch [53/60] Batch 200/1583                   Loss D: 0.0186, loss G: 5.4656\n",
      "Epoch [53/60] Batch 300/1583                   Loss D: 0.0482, loss G: 7.4452\n",
      "Epoch [53/60] Batch 400/1583                   Loss D: 0.0372, loss G: 4.9104\n",
      "Epoch [53/60] Batch 500/1583                   Loss D: 0.3731, loss G: 2.5949\n",
      "Epoch [53/60] Batch 600/1583                   Loss D: 0.0427, loss G: 4.8354\n",
      "Epoch [53/60] Batch 700/1583                   Loss D: 0.0378, loss G: 4.5450\n",
      "Epoch [53/60] Batch 800/1583                   Loss D: 0.0213, loss G: 4.8318\n",
      "Epoch [53/60] Batch 900/1583                   Loss D: 0.0368, loss G: 4.6554\n",
      "Epoch [53/60] Batch 1000/1583                   Loss D: 0.0342, loss G: 5.2142\n",
      "Epoch [53/60] Batch 1100/1583                   Loss D: 0.0292, loss G: 5.1669\n",
      "Epoch [53/60] Batch 1200/1583                   Loss D: 0.0383, loss G: 5.9628\n",
      "Epoch [53/60] Batch 1300/1583                   Loss D: 0.0292, loss G: 5.5021\n",
      "Epoch [53/60] Batch 1400/1583                   Loss D: 0.0255, loss G: 5.7819\n",
      "Epoch [53/60] Batch 1500/1583                   Loss D: 0.0377, loss G: 4.6209\n",
      "Epoch [54/60] Batch 0/1583                   Loss D: 1.6976, loss G: 0.5865\n",
      "Epoch [54/60] Batch 100/1583                   Loss D: 0.0339, loss G: 6.1076\n",
      "Epoch [54/60] Batch 200/1583                   Loss D: 0.0282, loss G: 5.1708\n",
      "Epoch [54/60] Batch 300/1583                   Loss D: 0.0537, loss G: 4.8017\n",
      "Epoch [54/60] Batch 400/1583                   Loss D: 0.0340, loss G: 5.2699\n",
      "Epoch [54/60] Batch 500/1583                   Loss D: 0.0357, loss G: 5.6211\n",
      "Epoch [54/60] Batch 600/1583                   Loss D: 0.0401, loss G: 6.7514\n",
      "Epoch [54/60] Batch 700/1583                   Loss D: 0.0359, loss G: 5.2088\n",
      "Epoch [54/60] Batch 800/1583                   Loss D: 0.0354, loss G: 4.1305\n",
      "Epoch [54/60] Batch 900/1583                   Loss D: 0.0406, loss G: 4.5781\n",
      "Epoch [54/60] Batch 1000/1583                   Loss D: 0.0652, loss G: 3.2821\n",
      "Epoch [54/60] Batch 1100/1583                   Loss D: 0.0363, loss G: 4.5705\n",
      "Epoch [54/60] Batch 1200/1583                   Loss D: 0.0178, loss G: 5.6537\n",
      "Epoch [54/60] Batch 1300/1583                   Loss D: 0.1012, loss G: 4.3659\n",
      "Epoch [54/60] Batch 1400/1583                   Loss D: 0.0458, loss G: 5.3879\n",
      "Epoch [54/60] Batch 1500/1583                   Loss D: 0.0386, loss G: 4.7699\n",
      "Epoch [55/60] Batch 0/1583                   Loss D: 0.0511, loss G: 4.7397\n",
      "Epoch [55/60] Batch 100/1583                   Loss D: 0.0295, loss G: 4.7524\n",
      "Epoch [55/60] Batch 200/1583                   Loss D: 0.0443, loss G: 6.3655\n",
      "Epoch [55/60] Batch 300/1583                   Loss D: 0.0992, loss G: 4.0938\n",
      "Epoch [55/60] Batch 400/1583                   Loss D: 0.2162, loss G: 2.6296\n",
      "Epoch [55/60] Batch 500/1583                   Loss D: 0.1606, loss G: 8.8448\n",
      "Epoch [55/60] Batch 600/1583                   Loss D: 0.0182, loss G: 4.8023\n",
      "Epoch [55/60] Batch 700/1583                   Loss D: 1.4354, loss G: 6.8523\n",
      "Epoch [55/60] Batch 800/1583                   Loss D: 0.0178, loss G: 4.9819\n",
      "Epoch [55/60] Batch 900/1583                   Loss D: 0.0113, loss G: 5.8516\n",
      "Epoch [55/60] Batch 1000/1583                   Loss D: 0.0197, loss G: 5.2614\n",
      "Epoch [55/60] Batch 1100/1583                   Loss D: 0.0231, loss G: 4.2089\n",
      "Epoch [55/60] Batch 1200/1583                   Loss D: 0.1455, loss G: 4.0319\n",
      "Epoch [55/60] Batch 1300/1583                   Loss D: 0.0246, loss G: 5.9258\n",
      "Epoch [55/60] Batch 1400/1583                   Loss D: 0.0147, loss G: 5.7863\n",
      "Epoch [55/60] Batch 1500/1583                   Loss D: 0.2603, loss G: 3.0301\n",
      "Epoch [56/60] Batch 0/1583                   Loss D: 0.0396, loss G: 5.2341\n",
      "Epoch [56/60] Batch 100/1583                   Loss D: 0.0287, loss G: 5.4771\n",
      "Epoch [56/60] Batch 200/1583                   Loss D: 0.0577, loss G: 4.6319\n",
      "Epoch [56/60] Batch 300/1583                   Loss D: 0.3253, loss G: 3.0200\n",
      "Epoch [56/60] Batch 400/1583                   Loss D: 0.0755, loss G: 3.8051\n",
      "Epoch [56/60] Batch 500/1583                   Loss D: 0.0559, loss G: 4.6430\n",
      "Epoch [56/60] Batch 600/1583                   Loss D: 0.1403, loss G: 3.5838\n",
      "Epoch [56/60] Batch 700/1583                   Loss D: 0.0165, loss G: 4.7464\n",
      "Epoch [56/60] Batch 800/1583                   Loss D: 0.0159, loss G: 5.6165\n",
      "Epoch [56/60] Batch 900/1583                   Loss D: 0.0249, loss G: 5.3538\n",
      "Epoch [56/60] Batch 1000/1583                   Loss D: 0.0753, loss G: 4.1093\n",
      "Epoch [56/60] Batch 1100/1583                   Loss D: 0.2027, loss G: 2.0211\n",
      "Epoch [56/60] Batch 1200/1583                   Loss D: 0.0364, loss G: 5.6399\n",
      "Epoch [56/60] Batch 1300/1583                   Loss D: 0.0568, loss G: 3.6118\n",
      "Epoch [56/60] Batch 1400/1583                   Loss D: 0.0101, loss G: 5.9292\n",
      "Epoch [56/60] Batch 1500/1583                   Loss D: 0.0563, loss G: 3.8008\n",
      "Epoch [57/60] Batch 0/1583                   Loss D: 0.0236, loss G: 6.1860\n",
      "Epoch [57/60] Batch 100/1583                   Loss D: 0.0558, loss G: 5.3150\n",
      "Epoch [57/60] Batch 200/1583                   Loss D: 0.0535, loss G: 5.9878\n",
      "Epoch [57/60] Batch 300/1583                   Loss D: 0.1233, loss G: 1.9240\n",
      "Epoch [57/60] Batch 400/1583                   Loss D: 0.0421, loss G: 5.0156\n",
      "Epoch [57/60] Batch 500/1583                   Loss D: 0.0149, loss G: 5.5499\n",
      "Epoch [57/60] Batch 600/1583                   Loss D: 0.0304, loss G: 4.6412\n",
      "Epoch [57/60] Batch 700/1583                   Loss D: 0.0765, loss G: 6.2695\n",
      "Epoch [57/60] Batch 800/1583                   Loss D: 0.0238, loss G: 5.9401\n",
      "Epoch [57/60] Batch 900/1583                   Loss D: 0.0614, loss G: 5.0040\n",
      "Epoch [57/60] Batch 1000/1583                   Loss D: 0.0246, loss G: 5.4270\n",
      "Epoch [57/60] Batch 1100/1583                   Loss D: 0.0618, loss G: 5.3034\n",
      "Epoch [57/60] Batch 1200/1583                   Loss D: 0.0240, loss G: 5.6504\n",
      "Epoch [57/60] Batch 1300/1583                   Loss D: 0.6817, loss G: 0.7572\n",
      "Epoch [57/60] Batch 1400/1583                   Loss D: 0.0493, loss G: 4.7015\n",
      "Epoch [57/60] Batch 1500/1583                   Loss D: 0.0465, loss G: 5.8781\n",
      "Epoch [58/60] Batch 0/1583                   Loss D: 0.1413, loss G: 2.0220\n",
      "Epoch [58/60] Batch 100/1583                   Loss D: 0.0582, loss G: 3.4980\n",
      "Epoch [58/60] Batch 200/1583                   Loss D: 0.0425, loss G: 5.1109\n",
      "Epoch [58/60] Batch 300/1583                   Loss D: 0.0280, loss G: 5.3309\n",
      "Epoch [58/60] Batch 400/1583                   Loss D: 0.0402, loss G: 5.0849\n",
      "Epoch [58/60] Batch 500/1583                   Loss D: 0.0298, loss G: 4.5677\n",
      "Epoch [58/60] Batch 600/1583                   Loss D: 0.0245, loss G: 6.5790\n",
      "Epoch [58/60] Batch 700/1583                   Loss D: 0.0564, loss G: 4.2155\n",
      "Epoch [58/60] Batch 800/1583                   Loss D: 0.0259, loss G: 5.4122\n",
      "Epoch [58/60] Batch 900/1583                   Loss D: 0.0346, loss G: 4.5251\n",
      "Epoch [58/60] Batch 1000/1583                   Loss D: 0.0226, loss G: 5.5555\n",
      "Epoch [58/60] Batch 1100/1583                   Loss D: 0.0298, loss G: 5.6053\n",
      "Epoch [58/60] Batch 1200/1583                   Loss D: 0.0300, loss G: 4.8357\n",
      "Epoch [58/60] Batch 1300/1583                   Loss D: 0.0332, loss G: 4.5881\n",
      "Epoch [58/60] Batch 1400/1583                   Loss D: 0.0136, loss G: 5.3921\n",
      "Epoch [58/60] Batch 1500/1583                   Loss D: 0.0657, loss G: 5.0081\n",
      "Epoch [59/60] Batch 0/1583                   Loss D: 0.0285, loss G: 5.5284\n",
      "Epoch [59/60] Batch 100/1583                   Loss D: 0.0086, loss G: 6.1833\n",
      "Epoch [59/60] Batch 200/1583                   Loss D: 1.5360, loss G: 1.4872\n",
      "Epoch [59/60] Batch 300/1583                   Loss D: 0.0273, loss G: 4.9274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/60] Batch 400/1583                   Loss D: 0.0863, loss G: 5.5390\n",
      "Epoch [59/60] Batch 500/1583                   Loss D: 0.0556, loss G: 4.0625\n",
      "Epoch [59/60] Batch 600/1583                   Loss D: 0.0435, loss G: 4.9962\n",
      "Epoch [59/60] Batch 700/1583                   Loss D: 0.0428, loss G: 5.2083\n",
      "Epoch [59/60] Batch 800/1583                   Loss D: 0.0376, loss G: 4.8994\n",
      "Epoch [59/60] Batch 900/1583                   Loss D: 0.0134, loss G: 5.5977\n",
      "Epoch [59/60] Batch 1000/1583                   Loss D: 0.0684, loss G: 4.6603\n",
      "Epoch [59/60] Batch 1100/1583                   Loss D: 0.3845, loss G: 3.3492\n",
      "Epoch [59/60] Batch 1200/1583                   Loss D: 1.5435, loss G: 0.1609\n",
      "Epoch [59/60] Batch 1300/1583                   Loss D: 0.1666, loss G: 8.7913\n",
      "Epoch [59/60] Batch 1400/1583                   Loss D: 0.0621, loss G: 4.9030\n",
      "Epoch [59/60] Batch 1500/1583                   Loss D: 0.0749, loss G: 4.5327\n"
     ]
    }
   ],
   "source": [
    "fixed_noise = torch.randn(32, noise_dim, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "gen.train()\n",
    "disc.train()\n",
    "for epoch in range(epochs):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(batch_size, noise_dim, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "       # Print losses occasionally and save a grid of generated fake images\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "\n",
    "                # Save a grid of generated fake images\n",
    "                fake_grid = vutils.make_grid(fake, normalize=True, padding=2, nrow=8)  # Adjust nrow as needed\n",
    "\n",
    "                image_filename = os.path.join(output_directory, f'fake_images_epoch{epoch}_batch{batch_idx}.png')\n",
    "                vutils.save_image(fake_grid, image_filename)\n",
    "\n",
    "                # Display real and fake images in TensorBoard\n",
    "                img_grid_real = vutils.make_grid(real[:32], normalize=True, padding=2, nrow=8)\n",
    "                img_grid_fake = vutils.make_grid(fake[:32], normalize=True, padding=2, nrow=8)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24c95fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(disc.state_dict(), \"Discriminator_celeb2\")\n",
    "torch.save(disc.state_dict(), \"Generator_celeb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2551f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
